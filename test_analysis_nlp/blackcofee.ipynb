{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/utsavagarwal/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import openpyxl\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd3 in position 184: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hy/f121tz693r385ldjzz60bxx40000gn/T/ipykernel_2980/1275852155.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# count+=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mcalculate_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Close the workbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hy/f121tz693r385ldjzz60bxx40000gn/T/ipykernel_2980/2703834378.py\u001b[0m in \u001b[0;36mcalculate_score\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Loop over paragraphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd3 in position 184: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the Excel file\n",
    "workbook = openpyxl.load_workbook('input.xlsx')\n",
    "\n",
    "# Select a specific worksheet\n",
    "worksheet = workbook['Sheet1']\n",
    "\n",
    "# Loop over columns with text or inputs\n",
    "count=0\n",
    "for column in worksheet.iter_cols(min_col=2, max_col=worksheet.max_column, values_only=True,min_row=2):\n",
    "    for url in column:\n",
    "        if url is not None:\n",
    "            # Perform operations on the cell value\n",
    "            # count+=1\n",
    "            print(url)\n",
    "            calculate_score(url)\n",
    "\n",
    "# Close the workbook\n",
    "workbook.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the directory path\n",
    "directory = './MasterDictionary'  # Replace with your directory path\n",
    "positive_score=0\n",
    "negative_score=0\n",
    "# Loop over file names in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if os.path.isfile(os.path.join(directory, filename)):\n",
    "        # Perform operations on the file\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='latin-1') as file:\n",
    "                text = file.read()\n",
    "                if(filename=='positive-words.txt'):\n",
    "                        positive_score= sum([1 for word in filter_paragraph if word in text])\n",
    "                else:\n",
    "                        negative_score= sum([-1 for word in filter_paragraph if word in text])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title_2: AI in healthcare to Improve Patient Outcomes\n",
      "[<p><strong>Introduction</strong></p>, <p>“If anything kills over 10 million people in the next few decades, it will be a highly infectious virus rather than a war. Not missiles but microbes.” Bill Gates’s remarks at a TED conference in 2014, right after the world had avoided the Ebola outbreak. When the new, unprecedented, invisible virus hit us, it met an overwhelmed and unprepared healthcare system and oblivious population. This public health emergency demonstrated our lack of scientific consideration and underlined the alarming need for robust innovations in our health and medical facilities. For the past few years, artificial intelligence has proven to be of tangible potential in the healthcare sectors, clinical practices, translational medical and biomedical research.</p>, <p>After the first case was detected in China on December 31st 2019, it was an AI program developed by BlueDot that alerted the world about the pandemic. It was quick to realise AI’s ability to analyse large chunks of data could help in detecting patterns and identifying and tracking the possible carriers of the virus.</p>, <p>Many tracing apps use AI to keep tabs on the people who have been infected and prevent the risk of cross-infection by using AI algorithms that can track patterns and extract some features to classify or categorise them.</p>, <p><strong>So how does AI do that?</strong></p>, <p>IBM Watson, a sophisticated AI that works on cloud computing and natural language processing, has prominently contributed to the healthcare sector on a global level. Being a conversational AI, since 2013, Watson has helped in recommending treatments to patients suffering from cancer to ensure that they get the best treatment at optimum costs.</p>, <p>Researchers at Google Inc. showed that an AI system can be trained on thousands of images to achieve physician-level sensitivity.</p>, <p>By identifying the molecular patterns associated with disease status and its subtypes, gene expression, and protein abundance levels, machine learning methods can detect fatal diseases like cancer at an early stage. Machine Learning (ML) techniques focus mainly on analyzing structured data, which can further help in clustering patients’ traits and infer the probability of disease outcomes. Since patient traits mainly include masses of data relating to age, gender, disease history, disease-specific data like diagnostic imaging and gene expressions, etc, ML can extract features from these data inputs by constructing data analytical algorithms.</p>, <p>ML algorithms are either supervised or unsupervised. Unsupervised learning helps in extracting features and clustering similar features together that further leads to early detection of diseases. Clustering and principal component analysis enable grouping or clustering of similar traits together that are further used to maximize or minimize the similarity between the patients within or between the clusters. Since patient traits are recorded in multiple dimensions, such as genes, principal component analysis(PCA) creates the apparatus to reduce these dimensions which humans could have not done alone.</p>, <p>Supervised learning considers the outcomes of the subjects together with the traits, and further correlates the inputs with the outputs to predict the probability of getting a particular clinical event, expected value of a disease level or expected survival time, or risk of Down’s syndrome.</p>, <p>Biomarker panels that are mostly used to detect ovarian cancer, have outperformed the conventional statistical methods due to machine learning. In addition to this, the use of EHRs and Bayesian networks, which are a part of supervised machine learning algorithms, can predict clinical outcomes and mortality respectively.</p>, <p>Unstructured data such as clinical notes and texts are converted into machine-readable structured data with the help of natural language processing(NLP). NLP works with two components: text processing and classification. Text processing helps in identifying a series of disease-relevant keywords in clinical notes and then through classification are further categorized into normal and abnormal cases. Chest screening through ML and NLP has helped find abnormalities in the lungs and provide treatment to covid patients. Healthcare organizations use NLP-based chatbots to increase interactions with patients, keeping their mental health and wellness in check.</p>, <p>Deep learning is a modern extension of the classical neural network techniques which helps explore more complex non-linear patterns in data, using algorithms like convolution neural network, recurrent neural network, deep belief network, and deep neural network which enables more accurate clinical prediction. When it comes to genome interpretation, deep neural networks surpass the conventional methods of logistics regression and support vector machines.</p>, <p>Sepsis Watch is an AI system trained in deep learning algorithms that holds the capability to analyze over 32 million data points to create a patient’s risk score and identify the early stages of sepsis.</p>, <p>Another method known as the Learning-based Optimization of the Under Sampling Pattern( LOUPE) is based on integrating full resolution MRI scans with the convolutional neural network algorithm, which helps in creating more accurate reconstructions.</p>, <p>Robotic surgery is widely considered in most delicate surgeries like gynaecology and prostate surgery. Even after striking the right balance between human decisions and AI precision, robotic surgery reduces surgeon efficiency as they have to be manually operated through a console. Thus, autonomous robotic surgery is on the rise with inventions such as robotic silicon fingers that mimic the sense of touch that surgeons need to identify organs, cut tissues, etc., or robotic catheters that can navigate whether it is touching blood, tissue, or valve.</p>, <p>Researchers at Children’s National Hospital, Washington have already developed an AI called Smart Tissue Autonomous Robot (STAR), which performs a colon anastomosis on its own with the help of an ML-powered suturing tool, that automatically detects the patient’s breathing pattern to apply suture at the correct point.</p>, <p>Cloud computing in healthcare has helped in retrieving and sharing medical records safely with a reduction in maintenance costs. Through this technology doctors and various healthcare workers have access to detailed patient data that helps in speeding up analysis ultimately leading to better care in the form of more accurate information, medications, and therapies.</p>, <p><strong>How can It help in Biomedical research?</strong></p>, <p>Since AI can analyze literature beyond readability, it can be used to concise biomedical research. With the help of ML algorithms and NLP, AI can accelerate screening and indexing of biomedical research, by ranking the literature of interest which allows researchers to formulate and test scientific hypotheses far more precisely and quickly. Taking it to the next level, AI systems like the computational modelling assistant (CMA) helps researchers to construct simulation models from the concepts they have in mind. Such innovations have majorly contributed to topics such as tumour suppressor mechanisms and protein-protein interaction information extraction.</p>, <p><strong>AI as precision medicine</strong></p>, <p>Since precision medicine focuses on healthcare interventions to individuals or groups of patients based on their profile, the various AI devices pave the way to practice it more efficiently. With the help of ML, complex algorithms like large datasets can be used to predict and create an optimal treatment strategy.</p>, <p>Deep learning and neural networks can be used to process data in healthcare apps and keep a close watch on the patient’s emotional state, food intake, or health monitoring. </p>, <p>“Omics” refers to the collective technologies that help in exploring the roles, relationships of various branches ending with the suffix “omics” such as genomics, proteomics, etc. Omics-based tests based on machine learning algorithms help find correlations and predict treatment responses, ultimately creating personalized treatments for individual patients. </p>, <p><strong>How it helps in psychology and neuro patients</strong></p>, <p>For psychologists studying creativity,  AI is promising new classes of experiments that are developing data structures and programs and exploring novel theories on a new horizon. Studies show that  AI can conduct therapy sessions, <a href=\"https://positivepsychology.com/e-therapy/\">e-therapy</a> sessions, and assessments autonomously, also assisting human practitioners before, during, or after sessions. The Detection and Computational Analysis of Psychological Signal project uses ML, computer vision, and NLP to analyze language, physical gestures, and social signals to identify cues for human distress. This ground-breaking technology assesses soldiers returning from combat and recognizes those who require further mental health support. In the future, it will combine data captured during face-to-face interviews with information on sleeping, eating, and online behaviours for a complete patient view.</p>, <p><strong>Stroke identification</strong></p>, <p>Stroke is another frequently occurring disease that affects more than 500 million people worldwide. Thrombus,  in the vessel cerebral infarction is the major (about 85%) cause of stroke occurrence. In recent years, AI techniques have been used in numerous stroke-related studies as early detection and timely treatment along with efficient outcome prediction can help solve the problem. With AI at our disposal, large amounts of data with rich information, more complications and real-life clinical questions can be addressed in this arena. Currently, two ML algorithms- genetic fuzzy finite state machine and PCA were implemented to build a model building solution. These include a human activity recognition stage and a stroke onset detection stage. An alert stroke message is activated as soon as a movement significantly different from the normal pattern is recorded. ML methods have been applied to neuroimaging data to assist disease evaluation and predicting stroke treatment for the diagnosis.</p>, <p><strong>Patient Monitoring</strong></p>, <p>Today, the market for AI-based patient monitoring is impressive and monetarily enticing. It is evolving with artificial sensors, smart technologies and explores everything from brain-computer interfaces to nanorobotics. Companies with their smart-watches have engaged people to perform remote monitoring even when they are not “patients”. An obvious place to start is with wearable and embedded sensors, glucose monitors, pulse monitors, oximeters, and ECG monitors. With patient monitoring becoming crucial, AI finds numerous applications in chronic conditions, intensive care units, operating rooms, emergency rooms, and cardiac wards where timeless clinical decision-making can be measured in seconds. More advances have started to gain traction like smart prosthetics and implants. These play an impeccable role in patient management post-surgery or rehabilitation. Demographics, laboratory results and vital signs can also be used to predict cardiac arrest, transfer into the intensive care unit, or even death. In addition, an interpretable machine-learning model can assist anesthesiologists in predicting hypoxaemia events during surgery. This suggests that with deep-learning algorithms, raw patient-monitoring data could be better used to avoid information overload and alert overload while enabling more accurate clinical prediction and timely decision-making.</p>, <p><strong> Conclusion</strong></p>, <p>Considering the vast range of tasks that an AI can do, it is evident that it holds deep potential in improving patient outcomes to skyrocketing levels. Using sophisticated algorithms AI can bring a revolution in the healthcare sector. Even after facing challenges like whether the technology will be able to deliver the promises, ethical measures, training physicians to use it, standard regulations etc, the role of AI in transforming the clinical practices cannot be ignored. The biggest challenge is the integration of AI in daily practice. All of these can be overcome and within that period the technologies will mature making the system far more enhanced and effective.</p>, <p class=\"tdm-descr\">We provide intelligence, accelerate innovation and implement technology with extraordinary breadth and depth global insights into the big data,data-driven dashboards, applications development, and information management for organizations through combining unique, specialist services and high-lvel human expertise.</p>, <p class=\"tdm-descr\">Contact us: <a href=\"mailto:hello@blackcoffer.com\">hello@blackcoffer.com</a></p>, <p class=\"tdm-descr\">© All Right Reserved, Blackcoffer(OPC) Pvt. Ltd</p>]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n",
    "# url = 'https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a Beautiful Soup object\n",
    "soup = BeautifulSoup(response.text, 'html5lib')\n",
    "\n",
    "# Find specific elements in the HTML\n",
    "# title = soup.title.text\n",
    "title_2=soup.find('h1')\n",
    "paragraphs = soup.find_all('p')\n",
    "paragraphs=paragraphs[16:]\n",
    "\n",
    "\n",
    "# Specify the directory path\n",
    "\n",
    "\n",
    "# Print the extracted information\n",
    "# print('Title:', title)\n",
    "print('Title_2:', title_2.get_text())\n",
    "print(paragraphs)\n",
    "# for p in paragraphs:\n",
    "    # print('-', p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(url):\n",
    "\n",
    "    # Make a request to the website\n",
    "    # url = 'https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code ==200:\n",
    "        # Create a Beautiful Soup object\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find specific elements in the HTML\n",
    "        title_2 = soup.find('h1').getText()\n",
    "        paragraphs = soup.find_all('p')\n",
    "        paragraphs = paragraphs[16:]\n",
    "\n",
    "        # Specify the directory path\n",
    "        directory = './StopWords'  # Replace with your directory path\n",
    "        filter_paragraph = []\n",
    "\n",
    "        # Loop over file names in the directory\n",
    "        for filename in os.listdir(directory):\n",
    "            if os.path.isfile(os.path.join(directory, filename)):\n",
    "                # Perform operations on the file\n",
    "                encodings = ['utf-8', 'latin-1']  # Add more encodings if needed\n",
    "                for encoding in encodings:\n",
    "                    try:\n",
    "                        with open(os.path.join(directory, filename), 'r', encoding=encoding) as file:\n",
    "                            text = file.read()\n",
    "                            for p in paragraphs:\n",
    "                                filter_word = [str(word) for word in p.get_text().split() if word.lower() not in text]\n",
    "                                filter_paragraph.append(' '.join(filter_word))\n",
    "                        break  # Break out of the loop if the file is read successfully\n",
    "                    except UnicodeDecodeError:\n",
    "                        print(f\"Unable to read {filename} with {encoding} encoding.\")\n",
    "\n",
    "        # print('Title_2:', title_2)\n",
    "        # print('Filtered Paragraphs:')\n",
    "        # for p in filter_paragraph:\n",
    "            # print('-', p)\n",
    "        no_of_words=0\n",
    "        for p in filter_paragraph:\n",
    "            no_of_words+=len(p.split())\n",
    "        print(no_of_words)\n",
    "\n",
    "        # Specify the directory path\n",
    "        directory = './MasterDictionary'  # Replace with your directory path\n",
    "        positive_score = 0\n",
    "        negative_score = 0\n",
    "\n",
    "        # Loop over file names in the directory\n",
    "        for filename in os.listdir(directory):\n",
    "            if os.path.isfile(os.path.join(directory, filename)):\n",
    "                # Perform operations on the file\n",
    "                with open(os.path.join(directory, filename), 'r', encoding='latin-1') as file:\n",
    "                    text = file.read()\n",
    "                    if filename == 'positive-words.txt':\n",
    "                        for p in filter_paragraph:\n",
    "                            positive_score += sum([1 for word in p.split() if word in text])\n",
    "                    else:\n",
    "                        for p in filter_paragraph:\n",
    "                            negative_score += sum([-1 for word in p.split() if word in text])\n",
    "        negative_score*=-1\n",
    "        # print(\"Positive score:\", positive_score)\n",
    "        # print(\"Negative score:\", negative_score)\n",
    "        polarity_score = ((positive_score - negative_score) / ((positive_score + negative_score) + 0.000001))\n",
    "        subjectivity_score =( (positive_score + negative_score)/ ((no_of_words) + 0.000001))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_score(url):\n",
    "    # Make a request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Create a Beautiful Soup object\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find specific elements in the HTML\n",
    "        title_2 = soup.find('h1').getText()\n",
    "        paragraphs = soup.find_all('p')\n",
    "        paragraphs = paragraphs[16:]\n",
    "\n",
    "        # Load stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # Cleaning using Stop Words Lists\n",
    "        filter_paragraph = []\n",
    "        stopwords_directory = './StopWords'\n",
    "        for filename in os.listdir(stopwords_directory):\n",
    "            if os.path.isfile(os.path.join(stopwords_directory, filename)):\n",
    "                with open(os.path.join(stopwords_directory, filename), 'r', encoding='utf-8') as file:\n",
    "                    stop_words.update(file.read().splitlines())\n",
    "\n",
    "        # Loop over paragraphs\n",
    "        for p in paragraphs:\n",
    "            words = word_tokenize(p.get_text())\n",
    "            filter_word = [word for word in words if word.lower() not in stop_words]\n",
    "            filter_paragraph.append(' '.join(filter_word))\n",
    "\n",
    "        # Calculate metrics\n",
    "        no_of_words = sum(len(p.split()) for p in filter_paragraph)\n",
    "        sentence_lengths = [len(sent.split()) for p in filter_paragraph for sent in sent_tokenize(p) if sent]\n",
    "        avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0\n",
    "        percentage_complex_words = (sum(1 for word in ' '.join(filter_paragraph).split() if count_syllables(word) > 2) / no_of_words) * 100 if no_of_words > 0 else 0\n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "        avg_words_per_sentence = no_of_words / len(filter_paragraph) if filter_paragraph else 0\n",
    "        complex_word_count = sum(1 for word in ' '.join(filter_paragraph).split() if count_syllables(word) > 2)\n",
    "        syllables = sum(count_syllables(word) for word in ' '.join(filter_paragraph).split())\n",
    "        syllables_per_word = syllables / no_of_words if no_of_words > 0 else 0\n",
    "\n",
    "        # Personal Pronouns\n",
    "        pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "        personal_pronouns_count = sum(len(re.findall(r\"\\b{}\\b\".format(pronoun), ' '.join(filter_paragraph), re.IGNORECASE)) for pronoun in pronouns)\n",
    "\n",
    "        # Average Word Length\n",
    "        avg_word_length = syllables / no_of_words if no_of_words > 0 else 0\n",
    "\n",
    "        # Create dictionaries for positive and negative words\n",
    "        positive_words = set()\n",
    "        negative_words = set()\n",
    "\n",
    "        # Creating a dictionary of Positive and Negative words\n",
    "        master_dictionary_directory = './MasterDictionary'\n",
    "        for filename in os.listdir(master_dictionary_directory):\n",
    "            if os.path.isfile(os.path.join(master_dictionary_directory, filename)):\n",
    "                with open(os.path.join(master_dictionary_directory, filename), 'r', encoding='latin-1') as file:\n",
    "                    words = file.read().splitlines()\n",
    "                    if filename == 'positive-words.txt':\n",
    "                        positive_words.update(words)\n",
    "                    else:\n",
    "                        negative_words.update(words)\n",
    "\n",
    "        # Extracting Derived variables\n",
    "        positive_score = sum(1 for word in ' '.join(filter_paragraph).split() if word in positive_words)\n",
    "        negative_score = sum(-1 for word in ' '.join(filter_paragraph).split() if word in negative_words)\n",
    "        negative_score *= -1\n",
    "        polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "        subjectivity_score = (positive_score + negative_score) / (no_of_words + 0.000001)\n",
    "\n",
    "        # Print results\n",
    "        print(\"Title_2:\", title_2)\n",
    "        print(\"Filtered Paragraphs:\")\n",
    "        for p in filter_paragraph:\n",
    "            print(\"-\", p)\n",
    "        \n",
    "        print(\"No. of Words:\", no_of_words)\n",
    "        print(\"Average Sentence Length:\", avg_sentence_length)\n",
    "        print(\"Percentage of Complex Words:\", percentage_complex_words)\n",
    "        print(\"FOG Index:\", fog_index)\n",
    "        print(\"Average Number of Words per Sentence:\", avg_words_per_sentence)\n",
    "        print(\"Complex Word Count:\", complex_word_count)\n",
    "        print(\"Syllables per Word:\", syllables_per_word)\n",
    "        print(\"Personal Pronouns Count:\", personal_pronouns_count)\n",
    "        print(\"Average Word Length:\", avg_word_length)\n",
    "        print(\"Positive score:\", positive_score)\n",
    "        print(\"Negative score:\", negative_score)\n",
    "        \n",
    "\n",
    "def count_syllables(word):\n",
    "    d = cmudict.dict()\n",
    "    word = word.lower()\n",
    "    if word not in d:\n",
    "        return 0\n",
    "    return max([len([y for y in x if y[-1].isdigit()]) for x in d[word]])\n",
    "\n",
    "\n",
    "# Test the function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to read StopWords_Currencies.txt with utf-8 encoding.\n",
      "11203\n"
     ]
    }
   ],
   "source": [
    "# def calculate_score(url):\n",
    "\n",
    "# Make a request to the website\n",
    "url = 'https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code ==200:\n",
    "    # Create a Beautiful Soup object\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find specific elements in the HTML\n",
    "    title_2 = soup.find('h1').getText()\n",
    "    paragraphs = soup.find_all('p')\n",
    "    paragraphs = paragraphs[16:]\n",
    "\n",
    "    # Specify the directory path\n",
    "    directory = './StopWords'  # Replace with your directory path\n",
    "    filter_paragraph = []\n",
    "\n",
    "    # Loop over file names in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            # Perform operations on the file\n",
    "            encodings = ['utf-8', 'latin-1']  # Add more encodings if needed\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    with open(os.path.join(directory, filename), 'r', encoding=encoding) as file:\n",
    "                        text = file.read()\n",
    "                        for p in paragraphs:\n",
    "                            filter_word = [str(word) for word in p.get_text().split() if word.lower() not in text]\n",
    "                            filter_paragraph.append(' '.join(filter_word))\n",
    "                    break  # Break out of the loop if the file is read successfully\n",
    "                except UnicodeDecodeError:\n",
    "                    print(f\"Unable to read {filename} with {encoding} encoding.\")\n",
    "\n",
    "    # print('Title_2:', title_2)\n",
    "    # print(len(filter_paragraph))\n",
    "    no_of_words=0\n",
    "    for p in filter_paragraph:\n",
    "        no_of_words+=len(p.split())\n",
    "    print(no_of_words)\n",
    "    # # Specify the directory path\n",
    "    # directory = './MasterDictionary'  # Replace with your directory path\n",
    "    # positive_score = 0\n",
    "    # negative_score = 0\n",
    "\n",
    "    # # Loop over file names in the directory\n",
    "    # for filename in os.listdir(directory):\n",
    "    #     if os.path.isfile(os.path.join(directory, filename)):\n",
    "    #         # Perform operations on the file\n",
    "    #         with open(os.path.join(directory, filename), 'r', encoding='latin-1') as file:\n",
    "    #             text = file.read()\n",
    "    #             if filename == 'positive-words.txt':\n",
    "    #                 for p in filter_paragraph:\n",
    "    #                     positive_score += sum([1 for word in p.split() if word in text])\n",
    "    #             else:\n",
    "    #                 for p in filter_paragraph:\n",
    "    #                     negative_score += sum([-1 for word in p.split() if word in text])\n",
    "    # negative_score*=-1\n",
    "    # # print(\"Positive score:\", positive_score)\n",
    "    # # print(\"Negative score:\", negative_score)\n",
    "    # polarity_score = ((positive_score - negative_score) / ((positive_score + negative_score) + 0.000001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in paragraphs:\n",
    "    filterword=[word for word in p.text.split() if word.lower() not in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/utsavagarwal/Downloads/drive-download-20230602T135742Z-001'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopWords_Names.txt\n",
      "StopWords_Auditor.txt\n",
      "StopWords_Geographic.txt\n",
      "StopWords_Generic.txt\n",
      "StopWords_Currencies.txt\n",
      "Unable to read StopWords_Currencies.txt with utf-8 encoding.\n",
      "StopWords_Currencies.txt\n",
      "StopWords_DatesandNumbers.txt\n",
      "StopWords_GenericLong.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory path\n",
    "directory = './StopWords'  # Replace with your directory path\n",
    "\n",
    "# Loop over file names in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if os.path.isfile(os.path.join(directory, filename)):\n",
    "        # Perform operations on the file\n",
    "        encodings = ['utf-8', 'latin-1']  # Add more encodings if needed\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(os.path.join(directory, filename), 'r', encoding=encoding) as file:\n",
    "                    print(filename)\n",
    "                    text = file.read()\n",
    "                # print(text)\n",
    "                break  # Break out of the loop if the file is read successfully\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Unable to read {filename} with {encoding} encoding.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pi'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"p\"+\"i\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERNST\n",
      "YOUNG\n",
      "DELOITTE\n",
      "TOUCHE\n",
      "KPMG\n",
      "PRICEWATERHOUSECOOPERS\n",
      "PRICEWATERHOUSE\n",
      "COOPERS\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/utsavagarwal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/\n",
      "Unable to read StopWords_Currencies.txt with utf-8 encoding.\n",
      "                                                 URL Positive score  \\\n",
      "0  https://insights.blackcoffer.com/ai-in-healthc...            476   \n",
      "\n",
      "  Negative score  Average Sentence Length  Percentage of Complex Words  \\\n",
      "0            182                18.860269                    30.768544   \n",
      "\n",
      "   FOG Index  Average Number of Words per Sentence Complex Word Count  \\\n",
      "0  19.851525                             45.726531               3447   \n",
      "\n",
      "   Syllables per Word Personal Pronouns Count  Average Word Length  \\\n",
      "0            2.061412                      19             2.061412   \n",
      "\n",
      "  clened words  No. of Words  \n",
      "0         9549       11203.0  \n",
      "https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/\n",
      "Unable to read StopWords_Currencies.txt with utf-8 encoding.\n",
      "                                                 URL Positive score  \\\n",
      "0  https://insights.blackcoffer.com/ai-in-healthc...            476   \n",
      "1  https://insights.blackcoffer.com/what-if-the-c...            383   \n",
      "\n",
      "  Negative score  Average Sentence Length  Percentage of Complex Words  \\\n",
      "0            182                18.860269                    30.768544   \n",
      "1            194                15.027682                    19.997697   \n",
      "\n",
      "   FOG Index  Average Number of Words per Sentence Complex Word Count  \\\n",
      "0  19.851525                             45.726531               3447   \n",
      "1  14.010152                             65.308271               1737   \n",
      "\n",
      "   Syllables per Word Personal Pronouns Count  Average Word Length  \\\n",
      "0            2.061412                      19             2.061412   \n",
      "1            1.775616                      46             1.775616   \n",
      "\n",
      "  clened words  No. of Words  \n",
      "0         9549       11203.0  \n",
      "1         6845        8686.0  \n",
      "https://insights.blackcoffer.com/what-jobs-will-robots-take-from-humans-in-the-future/\n",
      "Unable to read StopWords_Currencies.txt with utf-8 encoding.\n",
      "                                                 URL Positive score  \\\n",
      "0  https://insights.blackcoffer.com/ai-in-healthc...            476   \n",
      "1  https://insights.blackcoffer.com/what-if-the-c...            383   \n",
      "2  https://insights.blackcoffer.com/what-jobs-wil...            455   \n",
      "\n",
      "  Negative score  Average Sentence Length  Percentage of Complex Words  \\\n",
      "0            182                18.860269                    30.768544   \n",
      "1            194                15.027682                    19.997697   \n",
      "2            203                15.923896                    29.917798   \n",
      "\n",
      "   FOG Index  Average Number of Words per Sentence Complex Word Count  \\\n",
      "0  19.851525                             45.726531               3447   \n",
      "1  14.010152                             65.308271               1737   \n",
      "2  18.336678                             51.536946               3130   \n",
      "\n",
      "   Syllables per Word Personal Pronouns Count  Average Word Length  \\\n",
      "0            2.061412                      19             2.061412   \n",
      "1            1.775616                      46             1.775616   \n",
      "2            2.029344                      27             2.029344   \n",
      "\n",
      "  clened words  No. of Words  \n",
      "0         9549       11203.0  \n",
      "1         6845        8686.0  \n",
      "2         8503       10462.0  \n",
      "https://insights.blackcoffer.com/will-machine-replace-the-human-in-the-future-of-work/\n",
      "Unable to read StopWords_Currencies.txt with utf-8 encoding.\n",
      "                                                 URL Positive score  \\\n",
      "0  https://insights.blackcoffer.com/ai-in-healthc...            476   \n",
      "1  https://insights.blackcoffer.com/what-if-the-c...            383   \n",
      "2  https://insights.blackcoffer.com/what-jobs-wil...            455   \n",
      "3  https://insights.blackcoffer.com/will-machine-...            371   \n",
      "\n",
      "  Negative score  Average Sentence Length  Percentage of Complex Words  \\\n",
      "0            182                18.860269                    30.768544   \n",
      "1            194                15.027682                    19.997697   \n",
      "2            203                15.923896                    29.917798   \n",
      "3             97                13.199730                    20.611389   \n",
      "\n",
      "   FOG Index  Average Number of Words per Sentence Complex Word Count  \\\n",
      "0  19.851525                             45.726531               3447   \n",
      "1  14.010152                             65.308271               1737   \n",
      "2  18.336678                             51.536946               3130   \n",
      "3  13.524448                             43.665179               2016   \n",
      "\n",
      "   Syllables per Word Personal Pronouns Count  Average Word Length  \\\n",
      "0            2.061412                      19             2.061412   \n",
      "1            1.775616                      46             1.775616   \n",
      "2            2.029344                      27             2.029344   \n",
      "3            1.818321                      99             1.818321   \n",
      "\n",
      "  clened words  No. of Words  \n",
      "0         9549       11203.0  \n",
      "1         6845        8686.0  \n",
      "2         8503       10462.0  \n",
      "3         7389        9781.0  \n",
      "https://insights.blackcoffer.com/will-ai-replace-us-or-work-with-us/\n",
      "Unable to read StopWords_Currencies.txt with utf-8 encoding.\n",
      "                                                 URL Positive score  \\\n",
      "0  https://insights.blackcoffer.com/ai-in-healthc...            476   \n",
      "1  https://insights.blackcoffer.com/what-if-the-c...            383   \n",
      "2  https://insights.blackcoffer.com/what-jobs-wil...            455   \n",
      "3  https://insights.blackcoffer.com/will-machine-...            371   \n",
      "4  https://insights.blackcoffer.com/will-ai-repla...            350   \n",
      "\n",
      "  Negative score  Average Sentence Length  Percentage of Complex Words  \\\n",
      "0            182                18.860269                    30.768544   \n",
      "1            194                15.027682                    19.997697   \n",
      "2            203                15.923896                    29.917798   \n",
      "3             97                13.199730                    20.611389   \n",
      "4            131                17.447587                    23.221438   \n",
      "\n",
      "   FOG Index  Average Number of Words per Sentence Complex Word Count  \\\n",
      "0  19.851525                             45.726531               3447   \n",
      "1  14.010152                             65.308271               1737   \n",
      "2  18.336678                             51.536946               3130   \n",
      "3  13.524448                             43.665179               2016   \n",
      "4  16.267610                             46.812500               2435   \n",
      "\n",
      "   Syllables per Word Personal Pronouns Count  Average Word Length  \\\n",
      "0            2.061412                      19             2.061412   \n",
      "1            1.775616                      46             1.775616   \n",
      "2            2.029344                      27             2.029344   \n",
      "3            1.818321                      99             1.818321   \n",
      "4            1.863437                      92             1.863437   \n",
      "\n",
      "  clened words  No. of Words  \n",
      "0         9549       11203.0  \n",
      "1         6845        8686.0  \n",
      "2         8503       10462.0  \n",
      "3         7389        9781.0  \n",
      "4         8298       10486.0  \n",
      "https://insights.blackcoffer.com/man-and-machines-together-machines-are-more-diligent-than-humans-blackcoffe/\n",
      "Unable to read StopWords_Currencies.txt with utf-8 encoding.\n",
      "                                                 URL Positive score  \\\n",
      "0  https://insights.blackcoffer.com/ai-in-healthc...            476   \n",
      "1  https://insights.blackcoffer.com/what-if-the-c...            383   \n",
      "2  https://insights.blackcoffer.com/what-jobs-wil...            455   \n",
      "3  https://insights.blackcoffer.com/will-machine-...            371   \n",
      "4  https://insights.blackcoffer.com/will-ai-repla...            350   \n",
      "5  https://insights.blackcoffer.com/man-and-machi...            284   \n",
      "\n",
      "  Negative score  Average Sentence Length  Percentage of Complex Words  \\\n",
      "0            182                18.860269                    30.768544   \n",
      "1            194                15.027682                    19.997697   \n",
      "2            203                15.923896                    29.917798   \n",
      "3             97                13.199730                    20.611389   \n",
      "4            131                17.447587                    23.221438   \n",
      "5            112                18.254118                    21.564836   \n",
      "\n",
      "   FOG Index  Average Number of Words per Sentence Complex Word Count  \\\n",
      "0  19.851525                             45.726531               3447   \n",
      "1  14.010152                             65.308271               1737   \n",
      "2  18.336678                             51.536946               3130   \n",
      "3  13.524448                             43.665179               2016   \n",
      "4  16.267610                             46.812500               2435   \n",
      "5  15.927582                             61.571429               1673   \n",
      "\n",
      "   Syllables per Word Personal Pronouns Count  Average Word Length  \\\n",
      "0            2.061412                      19             2.061412   \n",
      "1            1.775616                      46             1.775616   \n",
      "2            2.029344                      27             2.029344   \n",
      "3            1.818321                      99             1.818321   \n",
      "4            1.863437                      92             1.863437   \n",
      "5            1.831915                     119             1.831915   \n",
      "\n",
      "  clened words  No. of Words  \n",
      "0         9549       11203.0  \n",
      "1         6845        8686.0  \n",
      "2         8503       10462.0  \n",
      "3         7389        9781.0  \n",
      "4         8298       10486.0  \n",
      "5         5993        7758.0  \n",
      "https://insights.blackcoffer.com/in-future-or-in-upcoming-years-humans-and-machines-are-going-to-work-together-in-every-field-of-work/\n",
      "Unable to read StopWords_Currencies.txt with utf-8 encoding.\n",
      "                                                 URL Positive score  \\\n",
      "0  https://insights.blackcoffer.com/ai-in-healthc...            476   \n",
      "1  https://insights.blackcoffer.com/what-if-the-c...            383   \n",
      "2  https://insights.blackcoffer.com/what-jobs-wil...            455   \n",
      "3  https://insights.blackcoffer.com/will-machine-...            371   \n",
      "4  https://insights.blackcoffer.com/will-ai-repla...            350   \n",
      "5  https://insights.blackcoffer.com/man-and-machi...            284   \n",
      "6  https://insights.blackcoffer.com/in-future-or-...            152   \n",
      "\n",
      "  Negative score  Average Sentence Length  Percentage of Complex Words  \\\n",
      "0            182                18.860269                    30.768544   \n",
      "1            194                15.027682                    19.997697   \n",
      "2            203                15.923896                    29.917798   \n",
      "3             97                13.199730                    20.611389   \n",
      "4            131                17.447587                    23.221438   \n",
      "5            112                18.254118                    21.564836   \n",
      "6             63                13.769006                    23.614355   \n",
      "\n",
      "   FOG Index  Average Number of Words per Sentence Complex Word Count  \\\n",
      "0  19.851525                             45.726531               3447   \n",
      "1  14.010152                             65.308271               1737   \n",
      "2  18.336678                             51.536946               3130   \n",
      "3  13.524448                             43.665179               2016   \n",
      "4  16.267610                             46.812500               2435   \n",
      "5  15.927582                             61.571429               1673   \n",
      "6  14.953345                             67.271429               1112   \n",
      "\n",
      "   Syllables per Word Personal Pronouns Count  Average Word Length  \\\n",
      "0            2.061412                      19             2.061412   \n",
      "1            1.775616                      46             1.775616   \n",
      "2            2.029344                      27             2.029344   \n",
      "3            1.818321                      99             1.818321   \n",
      "4            1.863437                      92             1.863437   \n",
      "5            1.831915                     119             1.831915   \n",
      "6            1.872797                      45             1.872797   \n",
      "\n",
      "  clened words  No. of Words  \n",
      "0         9549       11203.0  \n",
      "1         6845        8686.0  \n",
      "2         8503       10462.0  \n",
      "3         7389        9781.0  \n",
      "4         8298       10486.0  \n",
      "5         5993        7758.0  \n",
      "6         3820        4709.0  \n",
      "https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hy/f121tz693r385ldjzz60bxx40000gn/T/ipykernel_2980/3060540201.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hy/f121tz693r385ldjzz60bxx40000gn/T/ipykernel_2980/3060540201.py\u001b[0m in \u001b[0;36mcalculate_score\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Make a request to the website\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    585\u001b[0m         }\n\u001b[1;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    490\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_default_certs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1038\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def calculate_score(url):\n",
    "    # Make a request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Create a Beautiful Soup object\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find specific elements in the HTML\n",
    "        title_2 = soup.find('h1').getText()\n",
    "        paragraphs = soup.find_all('p')\n",
    "        paragraphs = paragraphs[16:]\n",
    "\n",
    "        # Load stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # Cleaning using Stop Words Lists\n",
    "        # filter_paragraph = []\n",
    "        # stopwords_directory = './StopWords'\n",
    "        # for filename in os.listdir(stopwords_directory):\n",
    "        #     if os.path.isfile(os.path.join(stopwords_directory, filename)):\n",
    "        #         with open(os.path.join(stopwords_directory, filename), 'r', encoding='latin-1') as file:\n",
    "        #             stop_words.update(file.read().splitlines())\n",
    "        directory = './StopWords'  # Replace with your directory path\n",
    "        filter_paragraph = []\n",
    "\n",
    "        # Loop over file names in the directory\n",
    "        for filename in os.listdir(directory):\n",
    "            if os.path.isfile(os.path.join(directory, filename)):\n",
    "                # Perform operations on the file\n",
    "                encodings = ['utf-8', 'latin-1']  # Add more encodings if needed\n",
    "                for encoding in encodings:\n",
    "                    try:\n",
    "                        with open(os.path.join(directory, filename), 'r', encoding=encoding) as file:\n",
    "                            text = file.read()\n",
    "                            for p in paragraphs:\n",
    "                                filter_word = [str(word) for word in p.get_text().split() if word.lower() not in text]\n",
    "                                filter_paragraph.append(' '.join(filter_word))\n",
    "                        break  # Break out of the loop if the file is read successfully\n",
    "                    except UnicodeDecodeError:\n",
    "                        print(f\"Unable to read {filename} with {encoding} encoding.\")\n",
    "\n",
    "        # Loop over paragraphs\n",
    "\n",
    "        # Calculate metrics\n",
    "        no_of_words = sum(len(p.split()) for p in filter_paragraph)\n",
    "        sentence_lengths = [len(sent.split()) for p in filter_paragraph for sent in sent_tokenize(p) if sent]\n",
    "        avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0\n",
    "        percentage_complex_words = (sum(1 for word in ' '.join(filter_paragraph).split() if count_syllables(word) > 2) / no_of_words) * 100 if no_of_words > 0 else 0\n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "        avg_words_per_sentence = no_of_words / len(filter_paragraph) if filter_paragraph else 0\n",
    "        complex_word_count = sum(1 for word in ' '.join(filter_paragraph).split() if count_syllables(word) > 2)\n",
    "        syllables = sum(count_syllables(word) for word in ' '.join(filter_paragraph).split())\n",
    "        syllables_per_word = syllables / no_of_words if no_of_words > 0 else 0\n",
    "\n",
    "        # Personal Pronouns\n",
    "        pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "        personal_pronouns_count = sum(len(re.findall(r\"\\b{}\\b\".format(pronoun), ' '.join(filter_paragraph), re.IGNORECASE)) for pronoun in pronouns)\n",
    "\n",
    "        # Average Word Length\n",
    "        avg_word_length = syllables / no_of_words if no_of_words > 0 else 0\n",
    "\n",
    "        # Create dictionaries for positive and negative words\n",
    "        positive_words = set()\n",
    "        negative_words = set()\n",
    "\n",
    "        # Creating a dictionary of Positive and Negative words\n",
    "        master_dictionary_directory = './MasterDictionary'\n",
    "        for filename in os.listdir(master_dictionary_directory):\n",
    "            if os.path.isfile(os.path.join(master_dictionary_directory, filename)):\n",
    "                with open(os.path.join(master_dictionary_directory, filename), 'r', encoding='latin-1') as file:\n",
    "                    words = file.read().splitlines()\n",
    "                    if filename == 'positive-words.txt':\n",
    "                        positive_words.update(words)\n",
    "                    else:\n",
    "                        negative_words.update(words)\n",
    "\n",
    "        # Extracting Derived variables\n",
    "        positive_score = sum(1 for word in ' '.join(filter_paragraph).split() if word in positive_words)\n",
    "        negative_score = sum(-1 for word in ' '.join(filter_paragraph).split() if word in negative_words)\n",
    "        negative_score *= -1\n",
    "        polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "        subjectivity_score = (positive_score + negative_score) / (no_of_words + 0.000001)\n",
    "        # print(\"Title_2:\", title_2)\n",
    "        # print(\"Filtered Paragraphs:\")\n",
    "        # for p in filter_paragraph:\n",
    "        #     print(\"-\", p)\n",
    "        \n",
    "        # print(\"No. of Words:\", no_of_words)\n",
    "        # print(\"Average Sentence Length:\", avg_sentence_length)\n",
    "        # print(\"Percentage of Complex Words:\", percentage_complex_words)\n",
    "        # print(\"FOG Index:\", fog_index)\n",
    "        # print(\"Average Number of Words per Sentence:\", avg_words_per_sentence)\n",
    "        # print(\"Complex Word Count:\", complex_word_count)\n",
    "        # print(\"Syllables per Word:\", syllables_per_word)\n",
    "        # print(\"Personal Pronouns Count:\", personal_pronouns_count)\n",
    "        # print(\"Average Word Length:\", avg_word_length)\n",
    "        # print(\"Positive score:\", positive_score)\n",
    "        # print(\"Negative score:\", negative_score)\n",
    "        \n",
    "        filter_paragraph_2=[]\n",
    "\n",
    "        for p in filter_paragraph:\n",
    "            words = word_tokenize(p)\n",
    "            filter_word = [word for word in words if word.lower() not in stop_words]\n",
    "            filter_paragraph_2.append(' '.join(filter_word))\n",
    "        clened_words = sum(len(p.split()) for p in filter_paragraph_2)\n",
    "        # print(\"Filtered Paragraphs(2):\")\n",
    "        # for p in filter_paragraph_2:\n",
    "        #     print(\"-\", p)\n",
    "        # Print results\n",
    "        # print(\"clened words\",clened_words)\n",
    "        data =pd.DataFrame( {\n",
    "             \"URL\":[url],\n",
    "            \"No. of Words\": [no_of_words],\n",
    "            \"Average Sentence Length\": [avg_sentence_length],\n",
    "            \"Percentage of Complex Words\": [percentage_complex_words],\n",
    "            \"FOG Index\": [fog_index],\n",
    "            \"Average Number of Words per Sentence\": [avg_words_per_sentence],\n",
    "            \"Complex Word Count\": [complex_word_count],\n",
    "            \"Syllables per Word\": [syllables_per_word],\n",
    "            \"Personal Pronouns Count\": [personal_pronouns_count],\n",
    "            \"Average Word Length\": [avg_word_length],\n",
    "            \"Positive score\": [positive_score],\n",
    "            \"Negative score\": [negative_score],\n",
    "            \"clened words\": [clened_words]\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        # print(\"No. of Words:\",np.nan)\n",
    "        # print(\"Average Sentence Length:\", np.nan)\n",
    "        # print(\"Percentage of Complex Words:\", np.nan)\n",
    "        # print(\"FOG Index:\", np.nan)\n",
    "        # print(\"Average Number of Words per Sentence:\", np.nan)\n",
    "        # print(\"Complex Word Count:\", np.nan)\n",
    "        # print(\"Syllables per Word:\", np.nan)\n",
    "        # print(\"Personal Pronouns Count:\", np.nan)\n",
    "        # print(\"Average Word Length:\", np.nan)\n",
    "        # print(\"Positive score:\", np.nan)\n",
    "        # print(\"Negative score:\", np.nan)\n",
    "        # print(\"clened words:\",np.nan)\n",
    "        data =pd.DataFrame( {\n",
    "             \"URL\":[url],\n",
    "            \"No. of Words\": [np.nan],\n",
    "            \"Average Sentence Length\": [np.nan],\n",
    "            \"Percentage of Complex Words\": [np.nan],\n",
    "            \"FOG Index\": [np.nan],\n",
    "            \"Average Number of Words per Sentence\": [np.nan],\n",
    "            \"Complex Word Count\": [np.nan],\n",
    "            \"Syllables per Word\": [np.nan],\n",
    "            \"Personal Pronouns Count\": [np.nan],\n",
    "            \"Average Word Length\": [np.nan],\n",
    "            \"Positive score\": [np.nan],\n",
    "            \"Negative score\": [np.nan],\n",
    "            \"clened words\": [np.nan]\n",
    "        })\n",
    "    return data\n",
    "\n",
    "\n",
    "def count_syllables(word):\n",
    "    vowels = 'aeiouy'\n",
    "    count = 0\n",
    "    prev_char = None\n",
    "    for char in word:\n",
    "        if char.lower() in vowels:\n",
    "            if prev_char is None or prev_char.lower() not in vowels:\n",
    "                count += 1\n",
    "        prev_char = char\n",
    "    if word.endswith(('es', 'ed')):\n",
    "        count -= 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# Test the function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the Excel file\n",
    "# Load the Excel file\n",
    "workbook = openpyxl.load_workbook('input.xlsx')\n",
    "\n",
    "# Select a specific worksheet\n",
    "worksheet = workbook['Sheet1']\n",
    "\n",
    "# Loop over rows with text or inputs\n",
    "output = pd.DataFrame(columns=[\"URL\", \"Positive score\", \"Negative score\", \"Average Sentence Length\",\n",
    "                               \"Percentage of Complex Words\", \"FOG Index\", \"Average Number of Words per Sentence\",\n",
    "                               \"Complex Word Count\", \"Syllables per Word\", \"Personal Pronouns Count\",\n",
    "                               \"Average Word Length\", \"clened words\"])\n",
    "for row in worksheet.iter_rows(min_row=2, max_row=worksheet.max_row, values_only=True):\n",
    "    url = row[1]  # Assuming the URL is in the second column (index 1)\n",
    "    if url is not None:\n",
    "        print(url)\n",
    "        x = calculate_score(url)\n",
    "        output = output.append(x, ignore_index=True)\n",
    "        print(output)\n",
    "# Close the workbook\n",
    "workbook.close()\n",
    "\n",
    "# Print the output DataFrame\n",
    "print(output)\n",
    "output.to_csv(\"output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Introduction']\n",
      "['“If', 'anything', 'kills', 'over', '10', 'million', 'people', 'the', 'next', 'few', 'decades,', 'it', 'will', 'be', 'highly', 'infectious', 'virus', 'rather', 'than', 'war.', 'Not', 'missiles', 'but', 'microbes.”', 'Bill', 'Gates’s', 'remarks', 'at', 'TED', 'conference', '2014,', 'right', 'after', 'the', 'world', 'had', 'avoided', 'the', 'Ebola', 'outbreak.', 'When', 'the', 'new,', 'unprecedented,', 'invisible', 'virus', 'hit', 'us,', 'it', 'met', 'an', 'overwhelmed', 'and', 'unprepared', 'healthcare', 'system', 'and', 'oblivious', 'population.', 'This', 'public', 'health', 'emergency', 'demonstrated', 'our', 'lack', 'scientific', 'consideration', 'and', 'underlined', 'the', 'alarming', 'need', 'for', 'robust', 'innovations', 'our', 'health', 'and', 'medical', 'facilities.', 'For', 'the', 'past', 'few', 'years,', 'artificial', 'intelligence', 'has', 'proven', 'to', 'be', 'tangible', 'potential', 'the', 'healthcare', 'sectors,', 'clinical', 'practices,', 'translational', 'medical', 'and', 'biomedical', 'research.']\n",
      "['After', 'the', 'case', 'was', 'detected', 'China', 'December', '31st', '2019,', 'it', 'was', 'an', 'AI', 'program', 'developed', 'by', 'BlueDot', 'that', 'alerted', 'the', 'world', 'about', 'the', 'pandemic.', 'It', 'was', 'quick', 'to', 'realise', 'AI’s', 'ability', 'to', 'analyse', 'large', 'chunks', 'data', 'could', 'help', 'detecting', 'patterns', 'and', 'identifying', 'and', 'tracking', 'the', 'possible', 'carriers', 'the', 'virus.']\n",
      "['Many', 'tracing', 'apps', 'use', 'AI', 'to', 'keep', 'tabs', 'the', 'people', 'who', 'have', 'been', 'infected', 'and', 'prevent', 'the', 'risk', 'cross-infection', 'by', 'using', 'AI', 'algorithms', 'that', 'can', 'track', 'patterns', 'and', 'extract', 'some', 'features', 'to', 'classify', 'categorise', 'them.']\n",
      "['So', 'how', 'does', 'AI', 'do', 'that?']\n",
      "['IBM', 'Watson,', 'sophisticated', 'AI', 'that', 'works', 'cloud', 'computing', 'and', 'natural', 'language', 'processing,', 'has', 'prominently', 'contributed', 'to', 'the', 'healthcare', 'sector', 'global', 'level.', 'Being', 'conversational', 'AI,', 'since', '2013,', 'Watson', 'has', 'helped', 'recommending', 'treatments', 'to', 'patients', 'suffering', 'cancer', 'to', 'ensure', 'that', 'they', 'get', 'the', 'best', 'treatment', 'at', 'optimum', 'costs.']\n",
      "['Researchers', 'at', 'Google', 'Inc.', 'showed', 'that', 'an', 'AI', 'system', 'can', 'be', 'trained', 'thousands', 'images', 'to', 'achieve', 'physician-level', 'sensitivity.']\n",
      "['By', 'identifying', 'the', 'molecular', 'patterns', 'associated', 'with', 'disease', 'status', 'and', 'its', 'subtypes,', 'expression,', 'and', 'protein', 'abundance', 'levels,', 'machine', 'learning', 'methods', 'can', 'detect', 'fatal', 'diseases', 'like', 'cancer', 'at', 'an', 'early', 'stage.', 'Machine', 'Learning', '(ML)', 'techniques', 'focus', 'mainly', 'analyzing', 'structured', 'data,', 'which', 'can', 'further', 'help', 'clustering', 'patients’', 'traits', 'and', 'infer', 'the', 'probability', 'disease', 'outcomes.', 'Since', 'patient', 'traits', 'mainly', 'include', 'masses', 'data', 'relating', 'to', 'age,', 'gender,', 'disease', 'history,', 'disease-specific', 'data', 'like', 'diagnostic', 'imaging', 'and', 'expressions,', 'etc,', 'ML', 'can', 'extract', 'features', 'these', 'data', 'inputs', 'by', 'constructing', 'data', 'analytical', 'algorithms.']\n",
      "['ML', 'algorithms', 'are', 'either', 'supervised', 'unsupervised.', 'Unsupervised', 'learning', 'helps', 'extracting', 'features', 'and', 'clustering', 'similar', 'features', 'together', 'that', 'further', 'leads', 'to', 'early', 'detection', 'diseases.', 'Clustering', 'and', 'principal', 'component', 'analysis', 'enable', 'grouping', 'clustering', 'similar', 'traits', 'together', 'that', 'are', 'further', 'used', 'to', 'maximize', 'minimize', 'the', 'similarity', 'between', 'the', 'patients', 'within', 'between', 'the', 'clusters.', 'Since', 'patient', 'traits', 'are', 'recorded', 'multiple', 'dimensions,', 'such', 'genes,', 'principal', 'component', 'analysis(PCA)', 'creates', 'the', 'apparatus', 'to', 'reduce', 'these', 'dimensions', 'which', 'humans', 'could', 'have', 'not', 'done', 'alone.']\n",
      "['Supervised', 'learning', 'considers', 'the', 'outcomes', 'the', 'subjects', 'together', 'with', 'the', 'traits,', 'and', 'further', 'correlates', 'the', 'inputs', 'with', 'the', 'outputs', 'to', 'predict', 'the', 'probability', 'getting', 'particular', 'clinical', 'event,', 'expected', 'value', 'disease', 'level', 'expected', 'survival', 'time,', 'risk', 'Down’s', 'syndrome.']\n",
      "['Biomarker', 'panels', 'that', 'are', 'mostly', 'used', 'to', 'detect', 'ovarian', 'cancer,', 'have', 'outperformed', 'the', 'conventional', 'statistical', 'methods', 'due', 'to', 'machine', 'learning.', 'addition', 'to', 'this,', 'the', 'use', 'EHRs', 'and', 'Bayesian', 'networks,', 'which', 'are', 'part', 'supervised', 'machine', 'learning', 'algorithms,', 'can', 'predict', 'clinical', 'outcomes', 'and', 'mortality', 'respectively.']\n",
      "['Unstructured', 'data', 'such', 'clinical', 'notes', 'and', 'texts', 'are', 'converted', 'into', 'machine-readable', 'structured', 'data', 'with', 'the', 'help', 'natural', 'language', 'processing(NLP).', 'NLP', 'works', 'with', 'two', 'components:', 'text', 'processing', 'and', 'classification.', 'Text', 'processing', 'helps', 'identifying', 'series', 'disease-relevant', 'keywords', 'clinical', 'notes', 'and', 'then', 'through', 'classification', 'are', 'further', 'categorized', 'into', 'normal', 'and', 'abnormal', 'cases.', 'Chest', 'screening', 'through', 'ML', 'and', 'NLP', 'has', 'helped', 'find', 'abnormalities', 'the', 'lungs', 'and', 'provide', 'treatment', 'to', 'covid', 'patients.', 'Healthcare', 'organizations', 'use', 'NLP-based', 'chatbots', 'to', 'increase', 'interactions', 'with', 'patients,', 'keeping', 'their', 'mental', 'health', 'and', 'wellness', 'check.']\n",
      "['Deep', 'learning', 'modern', 'extension', 'the', 'classical', 'neural', 'network', 'techniques', 'which', 'helps', 'explore', 'more', 'complex', 'non-linear', 'patterns', 'data,', 'using', 'algorithms', 'like', 'convolution', 'neural', 'network,', 'recurrent', 'neural', 'network,', 'deep', 'belief', 'network,', 'and', 'deep', 'neural', 'network', 'which', 'enables', 'more', 'accurate', 'clinical', 'prediction.', 'When', 'it', 'comes', 'to', 'genome', 'interpretation,', 'deep', 'neural', 'networks', 'surpass', 'the', 'conventional', 'methods', 'logistics', 'regression', 'and', 'support', 'vector', 'machines.']\n",
      "['Sepsis', 'Watch', 'an', 'AI', 'system', 'trained', 'deep', 'learning', 'algorithms', 'that', 'holds', 'the', 'capability', 'to', 'analyze', 'over', '32', 'million', 'data', 'points', 'to', 'create', 'patient’s', 'risk', 'score', 'and', 'identify', 'the', 'early', 'stages', 'sepsis.']\n",
      "['Another', 'method', 'known', 'the', 'Learning-based', 'Optimization', 'the', 'Under', 'Sampling', 'Pattern(', 'LOUPE)', 'based', 'integrating', 'full', 'resolution', 'MRI', 'scans', 'with', 'the', 'convolutional', 'neural', 'network', 'algorithm,', 'which', 'helps', 'creating', 'more', 'accurate', 'reconstructions.']\n",
      "['Robotic', 'surgery', 'widely', 'considered', 'delicate', 'surgeries', 'like', 'gynaecology', 'and', 'prostate', 'surgery.', 'Even', 'after', 'striking', 'the', 'right', 'balance', 'between', 'human', 'decisions', 'and', 'AI', 'precision,', 'robotic', 'surgery', 'reduces', 'surgeon', 'efficiency', 'they', 'have', 'to', 'be', 'manually', 'operated', 'through', 'console.', 'Thus,', 'autonomous', 'robotic', 'surgery', 'the', 'rise', 'with', 'inventions', 'such', 'robotic', 'silicon', 'fingers', 'that', 'mimic', 'the', 'sense', 'touch', 'that', 'surgeons', 'need', 'to', 'identify', 'organs,', 'cut', 'tissues,', 'etc.,', 'robotic', 'catheters', 'that', 'can', 'navigate', 'whether', 'it', 'touching', 'blood,', 'tissue,', 'valve.']\n",
      "['Researchers', 'at', 'Children’s', 'National', 'Hospital,', 'Washington', 'have', 'already', 'developed', 'an', 'AI', 'called', 'Smart', 'Tissue', 'Autonomous', 'Robot', '(STAR),', 'which', 'performs', 'colon', 'anastomosis', 'its', 'own', 'with', 'the', 'help', 'an', 'ML-powered', 'suturing', 'tool,', 'that', 'automatically', 'detects', 'the', 'patient’s', 'breathing', 'pattern', 'to', 'apply', 'suture', 'at', 'the', 'correct', 'point.']\n",
      "['Cloud', 'computing', 'healthcare', 'has', 'helped', 'retrieving', 'and', 'sharing', 'medical', 'records', 'safely', 'with', 'reduction', 'maintenance', 'costs.', 'Through', 'this', 'technology', 'doctors', 'and', 'various', 'healthcare', 'workers', 'have', 'access', 'to', 'detailed', 'patient', 'data', 'that', 'helps', 'speeding', 'up', 'analysis', 'ultimately', 'leading', 'to', 'better', 'care', 'the', 'form', 'more', 'accurate', 'information,', 'medications,', 'and', 'therapies.']\n",
      "['How', 'can', 'It', 'help', 'Biomedical', 'research?']\n",
      "['Since', 'AI', 'can', 'analyze', 'literature', 'beyond', 'readability,', 'it', 'can', 'be', 'used', 'to', 'concise', 'biomedical', 'research.', 'With', 'the', 'help', 'ML', 'algorithms', 'and', 'NLP,', 'AI', 'can', 'accelerate', 'screening', 'and', 'indexing', 'biomedical', 'research,', 'by', 'ranking', 'the', 'literature', 'interest', 'which', 'allows', 'researchers', 'to', 'formulate', 'and', 'test', 'scientific', 'hypotheses', 'far', 'more', 'precisely', 'and', 'quickly.', 'Taking', 'it', 'to', 'the', 'next', 'level,', 'AI', 'systems', 'like', 'the', 'computational', 'modelling', 'assistant', '(CMA)', 'helps', 'researchers', 'to', 'construct', 'simulation', 'models', 'the', 'concepts', 'they', 'have', 'mind.', 'Such', 'innovations', 'have', 'majorly', 'contributed', 'to', 'topics', 'such', 'tumour', 'suppressor', 'mechanisms', 'and', 'protein-protein', 'interaction', 'information', 'extraction.']\n",
      "['AI', 'precision', 'medicine']\n",
      "['Since', 'precision', 'medicine', 'focuses', 'healthcare', 'interventions', 'to', 'individuals', 'groups', 'patients', 'based', 'their', 'profile,', 'the', 'various', 'AI', 'devices', 'pave', 'the', 'way', 'to', 'practice', 'it', 'more', 'efficiently.', 'With', 'the', 'help', 'ML,', 'complex', 'algorithms', 'like', 'large', 'datasets', 'can', 'be', 'used', 'to', 'predict', 'and', 'create', 'an', 'optimal', 'treatment', 'strategy.']\n",
      "['Deep', 'learning', 'and', 'neural', 'networks', 'can', 'be', 'used', 'to', 'process', 'data', 'healthcare', 'apps', 'and', 'keep', 'close', 'watch', 'the', 'patient’s', 'emotional', 'state,', 'food', 'intake,', 'health', 'monitoring.']\n",
      "['“Omics”', 'refers', 'to', 'the', 'collective', 'technologies', 'that', 'help', 'exploring', 'the', 'roles,', 'relationships', 'various', 'branches', 'ending', 'with', 'the', 'suffix', '“omics”', 'such', 'genomics,', 'proteomics,', 'etc.', 'Omics-based', 'tests', 'based', 'machine', 'learning', 'algorithms', 'help', 'find', 'correlations', 'and', 'predict', 'treatment', 'responses,', 'ultimately', 'creating', 'personalized', 'treatments', 'for', 'individual', 'patients.']\n",
      "['How', 'it', 'helps', 'psychology', 'and', 'neuro', 'patients']\n",
      "['For', 'psychologists', 'studying', 'creativity,', 'AI', 'promising', 'new', 'classes', 'experiments', 'that', 'are', 'developing', 'data', 'structures', 'and', 'programs', 'and', 'exploring', 'novel', 'theories', 'new', 'horizon.', 'Studies', 'show', 'that', 'AI', 'can', 'conduct', 'therapy', 'sessions,', 'e-therapy', 'sessions,', 'and', 'assessments', 'autonomously,', 'also', 'assisting', 'human', 'practitioners', 'before,', 'during,', 'after', 'sessions.', 'The', 'Detection', 'and', 'Computational', 'Analysis', 'Psychological', 'Signal', 'project', 'uses', 'ML,', 'computer', 'vision,', 'and', 'NLP', 'to', 'analyze', 'language,', 'physical', 'gestures,', 'and', 'social', 'signals', 'to', 'identify', 'cues', 'for', 'human', 'distress.', 'This', 'ground-breaking', 'technology', 'assesses', 'soldiers', 'returning', 'combat', 'and', 'recognizes', 'those', 'who', 'require', 'further', 'mental', 'health', 'support.', 'the', 'future,', 'it', 'will', 'combine', 'data', 'captured', 'during', 'face-to-face', 'interviews', 'with', 'information', 'sleeping,', 'eating,', 'and', 'online', 'behaviours', 'for', 'complete', 'patient', 'view.']\n",
      "['Stroke', 'identification']\n",
      "['Stroke', 'another', 'frequently', 'occurring', 'disease', 'that', 'affects', 'more', 'than', '500', 'million', 'people', 'worldwide.', 'Thrombus,', 'the', 'vessel', 'cerebral', 'infarction', 'the', 'major', '(about', '85%)', 'cause', 'stroke', 'occurrence.', 'recent', 'years,', 'AI', 'techniques', 'have', 'been', 'used', 'numerous', 'stroke-related', 'studies', 'early', 'detection', 'and', 'timely', 'treatment', 'along', 'with', 'efficient', 'outcome', 'prediction', 'can', 'help', 'solve', 'the', 'problem.', 'With', 'AI', 'at', 'our', 'disposal,', 'large', 'amounts', 'data', 'with', 'rich', 'information,', 'more', 'complications', 'and', 'real-life', 'clinical', 'questions', 'can', 'be', 'addressed', 'this', 'arena.', 'Currently,', 'two', 'ML', 'algorithms-', 'genetic', 'fuzzy', 'finite', 'state', 'machine', 'and', 'PCA', 'were', 'implemented', 'to', 'build', 'model', 'building', 'solution.', 'These', 'include', 'human', 'activity', 'recognition', 'stage', 'and', 'stroke', 'onset', 'detection', 'stage.', 'An', 'alert', 'stroke', 'message', 'activated', 'soon', 'movement', 'significantly', 'different', 'the', 'normal', 'pattern', 'recorded.', 'ML', 'methods', 'have', 'been', 'applied', 'to', 'neuroimaging', 'data', 'to', 'assist', 'disease', 'evaluation', 'and', 'predicting', 'stroke', 'treatment', 'for', 'the', 'diagnosis.']\n",
      "['Patient', 'Monitoring']\n",
      "['Today,', 'the', 'market', 'for', 'AI-based', 'patient', 'monitoring', 'impressive', 'and', 'monetarily', 'enticing.', 'It', 'evolving', 'with', 'artificial', 'sensors,', 'smart', 'technologies', 'and', 'explores', 'everything', 'brain-computer', 'interfaces', 'to', 'nanorobotics.', 'Companies', 'with', 'their', 'smart-watches', 'have', 'engaged', 'people', 'to', 'perform', 'remote', 'monitoring', 'even', 'when', 'they', 'are', 'not', '“patients”.', 'An', 'obvious', 'place', 'to', 'start', 'with', 'wearable', 'and', 'embedded', 'sensors,', 'glucose', 'monitors,', 'pulse', 'monitors,', 'oximeters,', 'and', 'ECG', 'monitors.', 'With', 'patient', 'monitoring', 'becoming', 'crucial,', 'AI', 'finds', 'numerous', 'applications', 'chronic', 'conditions,', 'intensive', 'care', 'units,', 'operating', 'rooms,', 'emergency', 'rooms,', 'and', 'cardiac', 'wards', 'where', 'timeless', 'clinical', 'decision-making', 'can', 'be', 'measured', 'seconds.', 'More', 'advances', 'have', 'started', 'to', 'gain', 'traction', 'like', 'smart', 'prosthetics', 'and', 'implants.', 'These', 'play', 'an', 'impeccable', 'role', 'patient', 'management', 'post-surgery', 'rehabilitation.', 'Demographics,', 'laboratory', 'results', 'and', 'vital', 'signs', 'can', 'also', 'be', 'used', 'to', 'predict', 'cardiac', 'arrest,', 'transfer', 'into', 'the', 'intensive', 'care', 'unit,', 'even', 'death.', 'addition,', 'an', 'interpretable', 'machine-learning', 'model', 'can', 'assist', 'anesthesiologists', 'predicting', 'hypoxaemia', 'events', 'during', 'surgery.', 'This', 'suggests', 'that', 'with', 'deep-learning', 'algorithms,', 'raw', 'patient-monitoring', 'data', 'could', 'be', 'better', 'used', 'to', 'avoid', 'information', 'overload', 'and', 'alert', 'overload', 'while', 'enabling', 'more', 'accurate', 'clinical', 'prediction', 'and', 'timely', 'decision-making.']\n",
      "['Conclusion']\n",
      "['Considering', 'the', 'vast', 'range', 'tasks', 'that', 'an', 'AI', 'can', 'do,', 'it', 'evident', 'that', 'it', 'holds', 'deep', 'potential', 'improving', 'patient', 'outcomes', 'to', 'skyrocketing', 'levels.', 'Using', 'sophisticated', 'algorithms', 'AI', 'can', 'bring', 'revolution', 'the', 'healthcare', 'sector.', 'Even', 'after', 'facing', 'challenges', 'like', 'whether', 'the', 'technology', 'will', 'be', 'able', 'to', 'deliver', 'the', 'promises,', 'ethical', 'measures,', 'training', 'physicians', 'to', 'use', 'it,', 'standard', 'regulations', 'etc,', 'the', 'role', 'AI', 'transforming', 'the', 'clinical', 'practices', 'cannot', 'be', 'ignored.', 'The', 'biggest', 'challenge', 'the', 'integration', 'AI', 'daily', 'practice.', 'these', 'can', 'be', 'overcome', 'and', 'within', 'that', 'period', 'the', 'technologies', 'will', 'mature', 'making', 'the', 'system', 'far', 'more', 'enhanced', 'and', 'effective.']\n",
      "['We', 'provide', 'intelligence,', 'accelerate', 'innovation', 'and', 'implement', 'technology', 'with', 'extraordinary', 'breadth', 'and', 'depth', 'global', 'insights', 'into', 'the', 'big', 'data,data-driven', 'dashboards,', 'applications', 'development,', 'and', 'information', 'management', 'for', 'organizations', 'through', 'combining', 'unique,', 'specialist', 'services', 'and', 'high-lvel', 'human', 'expertise.']\n",
      "['Contact', 'us:', 'hello@blackcoffer.com']\n",
      "['©', 'Right', 'Reserved,', 'Blackcoffer(OPC)', 'Pvt.', 'Ltd']\n",
      "['Introduction']\n",
      "['“If', 'anything', 'kills', 'over', '10', 'million', 'people', 'in', 'the', 'next', 'few', 'decades,', 'it', 'will', 'be', 'a', 'highly', 'infectious', 'virus', 'rather', 'than', 'a', 'war.', 'Not', 'missiles', 'but', 'microbes.”', 'Bill', 'Gates’s', 'remarks', 'at', 'a', 'TED', 'conference', 'in', '2014,', 'right', 'after', 'the', 'world', 'had', 'avoided', 'the', 'Ebola', 'outbreak.', 'When', 'the', 'new,', 'unprecedented,', 'invisible', 'virus', 'hit', 'us,', 'it', 'met', 'an', 'overwhelmed', 'and', 'unprepared', 'healthcare', 'system', 'and', 'oblivious', 'population.', 'This', 'public', 'health', 'emergency', 'demonstrated', 'our', 'lack', 'of', 'scientific', 'consideration', 'and', 'underlined', 'the', 'alarming', 'need', 'for', 'robust', 'innovations', 'in', 'our', 'health', 'and', 'medical', 'facilities.', 'For', 'the', 'past', 'few', 'years,', 'artificial', 'intelligence', 'has', 'proven', 'to', 'be', 'of', 'tangible', 'potential', 'in', 'the', 'healthcare', 'sectors,', 'clinical', 'practices,', 'translational', 'medical', 'and', 'biomedical', 'research.']\n",
      "['After', 'the', 'first', 'case', 'was', 'detected', 'in', 'China', 'on', 'December', '31st', '2019,', 'it', 'was', 'an', 'AI', 'program', 'developed', 'by', 'BlueDot', 'that', 'alerted', 'the', 'world', 'about', 'the', 'pandemic.', 'It', 'was', 'quick', 'to', 'realise', 'AI’s', 'ability', 'to', 'analyse', 'large', 'chunks', 'of', 'data', 'could', 'help', 'in', 'detecting', 'patterns', 'and', 'identifying', 'and', 'tracking', 'the', 'possible', 'carriers', 'of', 'the', 'virus.']\n",
      "['Many', 'tracing', 'apps', 'use', 'AI', 'to', 'keep', 'tabs', 'on', 'the', 'people', 'who', 'have', 'been', 'infected', 'and', 'prevent', 'the', 'risk', 'of', 'cross-infection', 'by', 'using', 'AI', 'algorithms', 'that', 'can', 'track', 'patterns', 'and', 'extract', 'some', 'features', 'to', 'classify', 'or', 'categorise', 'them.']\n",
      "['So', 'how', 'does', 'AI', 'do', 'that?']\n",
      "['IBM', 'Watson,', 'a', 'sophisticated', 'AI', 'that', 'works', 'on', 'cloud', 'computing', 'and', 'natural', 'language', 'processing,', 'has', 'prominently', 'contributed', 'to', 'the', 'healthcare', 'sector', 'on', 'a', 'global', 'level.', 'Being', 'a', 'conversational', 'AI,', 'since', '2013,', 'Watson', 'has', 'helped', 'in', 'recommending', 'treatments', 'to', 'patients', 'suffering', 'from', 'cancer', 'to', 'ensure', 'that', 'they', 'get', 'the', 'best', 'treatment', 'at', 'optimum', 'costs.']\n",
      "['Researchers', 'at', 'Google', 'Inc.', 'showed', 'that', 'an', 'AI', 'system', 'can', 'be', 'trained', 'on', 'thousands', 'of', 'images', 'to', 'achieve', 'physician-level', 'sensitivity.']\n",
      "['By', 'identifying', 'the', 'molecular', 'patterns', 'associated', 'with', 'disease', 'status', 'and', 'its', 'subtypes,', 'gene', 'expression,', 'and', 'protein', 'abundance', 'levels,', 'machine', 'learning', 'methods', 'can', 'detect', 'fatal', 'diseases', 'like', 'cancer', 'at', 'an', 'early', 'stage.', 'Machine', 'Learning', '(ML)', 'techniques', 'focus', 'mainly', 'on', 'analyzing', 'structured', 'data,', 'which', 'can', 'further', 'help', 'in', 'clustering', 'patients’', 'traits', 'and', 'infer', 'the', 'probability', 'of', 'disease', 'outcomes.', 'Since', 'patient', 'traits', 'mainly', 'include', 'masses', 'of', 'data', 'relating', 'to', 'age,', 'gender,', 'disease', 'history,', 'disease-specific', 'data', 'like', 'diagnostic', 'imaging', 'and', 'gene', 'expressions,', 'etc,', 'ML', 'can', 'extract', 'features', 'from', 'these', 'data', 'inputs', 'by', 'constructing', 'data', 'analytical', 'algorithms.']\n",
      "['ML', 'algorithms', 'are', 'either', 'supervised', 'or', 'unsupervised.', 'Unsupervised', 'learning', 'helps', 'in', 'extracting', 'features', 'and', 'clustering', 'similar', 'features', 'together', 'that', 'further', 'leads', 'to', 'early', 'detection', 'of', 'diseases.', 'Clustering', 'and', 'principal', 'component', 'analysis', 'enable', 'grouping', 'or', 'clustering', 'of', 'similar', 'traits', 'together', 'that', 'are', 'further', 'used', 'to', 'maximize', 'or', 'minimize', 'the', 'similarity', 'between', 'the', 'patients', 'within', 'or', 'between', 'the', 'clusters.', 'Since', 'patient', 'traits', 'are', 'recorded', 'in', 'multiple', 'dimensions,', 'such', 'as', 'genes,', 'principal', 'component', 'analysis(PCA)', 'creates', 'the', 'apparatus', 'to', 'reduce', 'these', 'dimensions', 'which', 'humans', 'could', 'have', 'not', 'done', 'alone.']\n",
      "['Supervised', 'learning', 'considers', 'the', 'outcomes', 'of', 'the', 'subjects', 'together', 'with', 'the', 'traits,', 'and', 'further', 'correlates', 'the', 'inputs', 'with', 'the', 'outputs', 'to', 'predict', 'the', 'probability', 'of', 'getting', 'a', 'particular', 'clinical', 'event,', 'expected', 'value', 'of', 'a', 'disease', 'level', 'or', 'expected', 'survival', 'time,', 'or', 'risk', 'of', 'Down’s', 'syndrome.']\n",
      "['Biomarker', 'panels', 'that', 'are', 'mostly', 'used', 'to', 'detect', 'ovarian', 'cancer,', 'have', 'outperformed', 'the', 'conventional', 'statistical', 'methods', 'due', 'to', 'machine', 'learning.', 'In', 'addition', 'to', 'this,', 'the', 'use', 'of', 'EHRs', 'and', 'Bayesian', 'networks,', 'which', 'are', 'a', 'part', 'of', 'supervised', 'machine', 'learning', 'algorithms,', 'can', 'predict', 'clinical', 'outcomes', 'and', 'mortality', 'respectively.']\n",
      "['Unstructured', 'data', 'such', 'as', 'clinical', 'notes', 'and', 'texts', 'are', 'converted', 'into', 'machine-readable', 'structured', 'data', 'with', 'the', 'help', 'of', 'natural', 'language', 'processing(NLP).', 'NLP', 'works', 'with', 'two', 'components:', 'text', 'processing', 'and', 'classification.', 'Text', 'processing', 'helps', 'in', 'identifying', 'a', 'series', 'of', 'disease-relevant', 'keywords', 'in', 'clinical', 'notes', 'and', 'then', 'through', 'classification', 'are', 'further', 'categorized', 'into', 'normal', 'and', 'abnormal', 'cases.', 'Chest', 'screening', 'through', 'ML', 'and', 'NLP', 'has', 'helped', 'find', 'abnormalities', 'in', 'the', 'lungs', 'and', 'provide', 'treatment', 'to', 'covid', 'patients.', 'Healthcare', 'organizations', 'use', 'NLP-based', 'chatbots', 'to', 'increase', 'interactions', 'with', 'patients,', 'keeping', 'their', 'mental', 'health', 'and', 'wellness', 'in', 'check.']\n",
      "['Deep', 'learning', 'is', 'a', 'modern', 'extension', 'of', 'the', 'classical', 'neural', 'network', 'techniques', 'which', 'helps', 'explore', 'more', 'complex', 'non-linear', 'patterns', 'in', 'data,', 'using', 'algorithms', 'like', 'convolution', 'neural', 'network,', 'recurrent', 'neural', 'network,', 'deep', 'belief', 'network,', 'and', 'deep', 'neural', 'network', 'which', 'enables', 'more', 'accurate', 'clinical', 'prediction.', 'When', 'it', 'comes', 'to', 'genome', 'interpretation,', 'deep', 'neural', 'networks', 'surpass', 'the', 'conventional', 'methods', 'of', 'logistics', 'regression', 'and', 'support', 'vector', 'machines.']\n",
      "['Sepsis', 'Watch', 'is', 'an', 'AI', 'system', 'trained', 'in', 'deep', 'learning', 'algorithms', 'that', 'holds', 'the', 'capability', 'to', 'analyze', 'over', '32', 'million', 'data', 'points', 'to', 'create', 'a', 'patient’s', 'risk', 'score', 'and', 'identify', 'the', 'early', 'stages', 'of', 'sepsis.']\n",
      "['Another', 'method', 'known', 'as', 'the', 'Learning-based', 'Optimization', 'of', 'the', 'Under', 'Sampling', 'Pattern(', 'LOUPE)', 'is', 'based', 'on', 'integrating', 'full', 'resolution', 'MRI', 'scans', 'with', 'the', 'convolutional', 'neural', 'network', 'algorithm,', 'which', 'helps', 'in', 'creating', 'more', 'accurate', 'reconstructions.']\n",
      "['Robotic', 'surgery', 'is', 'widely', 'considered', 'in', 'most', 'delicate', 'surgeries', 'like', 'gynaecology', 'and', 'prostate', 'surgery.', 'Even', 'after', 'striking', 'the', 'right', 'balance', 'between', 'human', 'decisions', 'and', 'AI', 'precision,', 'robotic', 'surgery', 'reduces', 'surgeon', 'efficiency', 'as', 'they', 'have', 'to', 'be', 'manually', 'operated', 'through', 'a', 'console.', 'Thus,', 'autonomous', 'robotic', 'surgery', 'is', 'on', 'the', 'rise', 'with', 'inventions', 'such', 'as', 'robotic', 'silicon', 'fingers', 'that', 'mimic', 'the', 'sense', 'of', 'touch', 'that', 'surgeons', 'need', 'to', 'identify', 'organs,', 'cut', 'tissues,', 'etc.,', 'or', 'robotic', 'catheters', 'that', 'can', 'navigate', 'whether', 'it', 'is', 'touching', 'blood,', 'tissue,', 'or', 'valve.']\n",
      "['Researchers', 'at', 'Children’s', 'National', 'Hospital,', 'Washington', 'have', 'already', 'developed', 'an', 'AI', 'called', 'Smart', 'Tissue', 'Autonomous', 'Robot', '(STAR),', 'which', 'performs', 'a', 'colon', 'anastomosis', 'on', 'its', 'own', 'with', 'the', 'help', 'of', 'an', 'ML-powered', 'suturing', 'tool,', 'that', 'automatically', 'detects', 'the', 'patient’s', 'breathing', 'pattern', 'to', 'apply', 'suture', 'at', 'the', 'correct', 'point.']\n",
      "['Cloud', 'computing', 'in', 'healthcare', 'has', 'helped', 'in', 'retrieving', 'and', 'sharing', 'medical', 'records', 'safely', 'with', 'a', 'reduction', 'in', 'maintenance', 'costs.', 'Through', 'this', 'technology', 'doctors', 'and', 'various', 'healthcare', 'workers', 'have', 'access', 'to', 'detailed', 'patient', 'data', 'that', 'helps', 'in', 'speeding', 'up', 'analysis', 'ultimately', 'leading', 'to', 'better', 'care', 'in', 'the', 'form', 'of', 'more', 'accurate', 'information,', 'medications,', 'and', 'therapies.']\n",
      "['How', 'can', 'It', 'help', 'in', 'Biomedical', 'research?']\n",
      "['Since', 'AI', 'can', 'analyze', 'literature', 'beyond', 'readability,', 'it', 'can', 'be', 'used', 'to', 'concise', 'biomedical', 'research.', 'With', 'the', 'help', 'of', 'ML', 'algorithms', 'and', 'NLP,', 'AI', 'can', 'accelerate', 'screening', 'and', 'indexing', 'of', 'biomedical', 'research,', 'by', 'ranking', 'the', 'literature', 'of', 'interest', 'which', 'allows', 'researchers', 'to', 'formulate', 'and', 'test', 'scientific', 'hypotheses', 'far', 'more', 'precisely', 'and', 'quickly.', 'Taking', 'it', 'to', 'the', 'next', 'level,', 'AI', 'systems', 'like', 'the', 'computational', 'modelling', 'assistant', '(CMA)', 'helps', 'researchers', 'to', 'construct', 'simulation', 'models', 'from', 'the', 'concepts', 'they', 'have', 'in', 'mind.', 'Such', 'innovations', 'have', 'majorly', 'contributed', 'to', 'topics', 'such', 'as', 'tumour', 'suppressor', 'mechanisms', 'and', 'protein-protein', 'interaction', 'information', 'extraction.']\n",
      "['AI', 'as', 'precision', 'medicine']\n",
      "['Since', 'precision', 'medicine', 'focuses', 'on', 'healthcare', 'interventions', 'to', 'individuals', 'or', 'groups', 'of', 'patients', 'based', 'on', 'their', 'profile,', 'the', 'various', 'AI', 'devices', 'pave', 'the', 'way', 'to', 'practice', 'it', 'more', 'efficiently.', 'With', 'the', 'help', 'of', 'ML,', 'complex', 'algorithms', 'like', 'large', 'datasets', 'can', 'be', 'used', 'to', 'predict', 'and', 'create', 'an', 'optimal', 'treatment', 'strategy.']\n",
      "['Deep', 'learning', 'and', 'neural', 'networks', 'can', 'be', 'used', 'to', 'process', 'data', 'in', 'healthcare', 'apps', 'and', 'keep', 'a', 'close', 'watch', 'on', 'the', 'patient’s', 'emotional', 'state,', 'food', 'intake,', 'or', 'health', 'monitoring.']\n",
      "['“Omics”', 'refers', 'to', 'the', 'collective', 'technologies', 'that', 'help', 'in', 'exploring', 'the', 'roles,', 'relationships', 'of', 'various', 'branches', 'ending', 'with', 'the', 'suffix', '“omics”', 'such', 'as', 'genomics,', 'proteomics,', 'etc.', 'Omics-based', 'tests', 'based', 'on', 'machine', 'learning', 'algorithms', 'help', 'find', 'correlations', 'and', 'predict', 'treatment', 'responses,', 'ultimately', 'creating', 'personalized', 'treatments', 'for', 'individual', 'patients.']\n",
      "['How', 'it', 'helps', 'in', 'psychology', 'and', 'neuro', 'patients']\n",
      "['For', 'psychologists', 'studying', 'creativity,', 'AI', 'is', 'promising', 'new', 'classes', 'of', 'experiments', 'that', 'are', 'developing', 'data', 'structures', 'and', 'programs', 'and', 'exploring', 'novel', 'theories', 'on', 'a', 'new', 'horizon.', 'Studies', 'show', 'that', 'AI', 'can', 'conduct', 'therapy', 'sessions,', 'e-therapy', 'sessions,', 'and', 'assessments', 'autonomously,', 'also', 'assisting', 'human', 'practitioners', 'before,', 'during,', 'or', 'after', 'sessions.', 'The', 'Detection', 'and', 'Computational', 'Analysis', 'of', 'Psychological', 'Signal', 'project', 'uses', 'ML,', 'computer', 'vision,', 'and', 'NLP', 'to', 'analyze', 'language,', 'physical', 'gestures,', 'and', 'social', 'signals', 'to', 'identify', 'cues', 'for', 'human', 'distress.', 'This', 'ground-breaking', 'technology', 'assesses', 'soldiers', 'returning', 'from', 'combat', 'and', 'recognizes', 'those', 'who', 'require', 'further', 'mental', 'health', 'support.', 'In', 'the', 'future,', 'it', 'will', 'combine', 'data', 'captured', 'during', 'face-to-face', 'interviews', 'with', 'information', 'on', 'sleeping,', 'eating,', 'and', 'online', 'behaviours', 'for', 'a', 'complete', 'patient', 'view.']\n",
      "['Stroke', 'identification']\n",
      "['Stroke', 'is', 'another', 'frequently', 'occurring', 'disease', 'that', 'affects', 'more', 'than', '500', 'million', 'people', 'worldwide.', 'Thrombus,', 'in', 'the', 'vessel', 'cerebral', 'infarction', 'is', 'the', 'major', '(about', '85%)', 'cause', 'of', 'stroke', 'occurrence.', 'In', 'recent', 'years,', 'AI', 'techniques', 'have', 'been', 'used', 'in', 'numerous', 'stroke-related', 'studies', 'as', 'early', 'detection', 'and', 'timely', 'treatment', 'along', 'with', 'efficient', 'outcome', 'prediction', 'can', 'help', 'solve', 'the', 'problem.', 'With', 'AI', 'at', 'our', 'disposal,', 'large', 'amounts', 'of', 'data', 'with', 'rich', 'information,', 'more', 'complications', 'and', 'real-life', 'clinical', 'questions', 'can', 'be', 'addressed', 'in', 'this', 'arena.', 'Currently,', 'two', 'ML', 'algorithms-', 'genetic', 'fuzzy', 'finite', 'state', 'machine', 'and', 'PCA', 'were', 'implemented', 'to', 'build', 'a', 'model', 'building', 'solution.', 'These', 'include', 'a', 'human', 'activity', 'recognition', 'stage', 'and', 'a', 'stroke', 'onset', 'detection', 'stage.', 'An', 'alert', 'stroke', 'message', 'is', 'activated', 'as', 'soon', 'as', 'a', 'movement', 'significantly', 'different', 'from', 'the', 'normal', 'pattern', 'is', 'recorded.', 'ML', 'methods', 'have', 'been', 'applied', 'to', 'neuroimaging', 'data', 'to', 'assist', 'disease', 'evaluation', 'and', 'predicting', 'stroke', 'treatment', 'for', 'the', 'diagnosis.']\n",
      "['Patient', 'Monitoring']\n",
      "['Today,', 'the', 'market', 'for', 'AI-based', 'patient', 'monitoring', 'is', 'impressive', 'and', 'monetarily', 'enticing.', 'It', 'is', 'evolving', 'with', 'artificial', 'sensors,', 'smart', 'technologies', 'and', 'explores', 'everything', 'from', 'brain-computer', 'interfaces', 'to', 'nanorobotics.', 'Companies', 'with', 'their', 'smart-watches', 'have', 'engaged', 'people', 'to', 'perform', 'remote', 'monitoring', 'even', 'when', 'they', 'are', 'not', '“patients”.', 'An', 'obvious', 'place', 'to', 'start', 'is', 'with', 'wearable', 'and', 'embedded', 'sensors,', 'glucose', 'monitors,', 'pulse', 'monitors,', 'oximeters,', 'and', 'ECG', 'monitors.', 'With', 'patient', 'monitoring', 'becoming', 'crucial,', 'AI', 'finds', 'numerous', 'applications', 'in', 'chronic', 'conditions,', 'intensive', 'care', 'units,', 'operating', 'rooms,', 'emergency', 'rooms,', 'and', 'cardiac', 'wards', 'where', 'timeless', 'clinical', 'decision-making', 'can', 'be', 'measured', 'in', 'seconds.', 'More', 'advances', 'have', 'started', 'to', 'gain', 'traction', 'like', 'smart', 'prosthetics', 'and', 'implants.', 'These', 'play', 'an', 'impeccable', 'role', 'in', 'patient', 'management', 'post-surgery', 'or', 'rehabilitation.', 'Demographics,', 'laboratory', 'results', 'and', 'vital', 'signs', 'can', 'also', 'be', 'used', 'to', 'predict', 'cardiac', 'arrest,', 'transfer', 'into', 'the', 'intensive', 'care', 'unit,', 'or', 'even', 'death.', 'In', 'addition,', 'an', 'interpretable', 'machine-learning', 'model', 'can', 'assist', 'anesthesiologists', 'in', 'predicting', 'hypoxaemia', 'events', 'during', 'surgery.', 'This', 'suggests', 'that', 'with', 'deep-learning', 'algorithms,', 'raw', 'patient-monitoring', 'data', 'could', 'be', 'better', 'used', 'to', 'avoid', 'information', 'overload', 'and', 'alert', 'overload', 'while', 'enabling', 'more', 'accurate', 'clinical', 'prediction', 'and', 'timely', 'decision-making.']\n",
      "['Conclusion']\n",
      "['Considering', 'the', 'vast', 'range', 'of', 'tasks', 'that', 'an', 'AI', 'can', 'do,', 'it', 'is', 'evident', 'that', 'it', 'holds', 'deep', 'potential', 'in', 'improving', 'patient', 'outcomes', 'to', 'skyrocketing', 'levels.', 'Using', 'sophisticated', 'algorithms', 'AI', 'can', 'bring', 'a', 'revolution', 'in', 'the', 'healthcare', 'sector.', 'Even', 'after', 'facing', 'challenges', 'like', 'whether', 'the', 'technology', 'will', 'be', 'able', 'to', 'deliver', 'the', 'promises,', 'ethical', 'measures,', 'training', 'physicians', 'to', 'use', 'it,', 'standard', 'regulations', 'etc,', 'the', 'role', 'of', 'AI', 'in', 'transforming', 'the', 'clinical', 'practices', 'cannot', 'be', 'ignored.', 'The', 'biggest', 'challenge', 'is', 'the', 'integration', 'of', 'AI', 'in', 'daily', 'practice.', 'All', 'of', 'these', 'can', 'be', 'overcome', 'and', 'within', 'that', 'period', 'the', 'technologies', 'will', 'mature', 'making', 'the', 'system', 'far', 'more', 'enhanced', 'and', 'effective.']\n",
      "['We', 'provide', 'intelligence,', 'accelerate', 'innovation', 'and', 'implement', 'technology', 'with', 'extraordinary', 'breadth', 'and', 'depth', 'global', 'insights', 'into', 'the', 'big', 'data,data-driven', 'dashboards,', 'applications', 'development,', 'and', 'information', 'management', 'for', 'organizations', 'through', 'combining', 'unique,', 'specialist', 'services', 'and', 'high-lvel', 'human', 'expertise.']\n",
      "['Contact', 'us:', 'hello@blackcoffer.com']\n",
      "['©', 'All', 'Right', 'Reserved,', 'Blackcoffer(OPC)', 'Pvt.', 'Ltd']\n",
      "['Introduction']\n",
      "['“If', 'anything', 'kills', 'over', '10', 'million', 'people', 'in', 'the', 'next', 'few', 'decades,', 'will', 'be', 'highly', 'infectious', 'virus', 'rather', 'than', 'war.', 'Not', 'missiles', 'but', 'microbes.”', 'Bill', 'Gates’s', 'remarks', 'TED', 'conference', 'in', '2014,', 'right', 'after', 'the', 'world', 'had', 'avoided', 'the', 'Ebola', 'outbreak.', 'When', 'the', 'new,', 'unprecedented,', 'invisible', 'virus', 'hit', 'us,', 'met', 'an', 'overwhelmed', 'and', 'unprepared', 'healthcare', 'system', 'and', 'oblivious', 'population.', 'This', 'public', 'health', 'emergency', 'demonstrated', 'our', 'lack', 'of', 'scientific', 'consideration', 'and', 'underlined', 'the', 'alarming', 'need', 'for', 'robust', 'innovations', 'in', 'our', 'health', 'and', 'medical', 'facilities.', 'For', 'the', 'past', 'few', 'years,', 'artificial', 'intelligence', 'has', 'proven', 'to', 'be', 'of', 'tangible', 'potential', 'in', 'the', 'healthcare', 'sectors,', 'clinical', 'practices,', 'translational', 'medical', 'and', 'biomedical', 'research.']\n",
      "['After', 'the', 'first', 'case', 'was', 'detected', 'in', 'China', 'on', 'December', '31st', '2019,', 'was', 'an', 'AI', 'program', 'developed', 'by', 'BlueDot', 'that', 'alerted', 'the', 'world', 'about', 'the', 'pandemic.', 'was', 'quick', 'to', 'realise', 'AI’s', 'ability', 'to', 'analyse', 'large', 'chunks', 'of', 'data', 'could', 'help', 'in', 'detecting', 'patterns', 'and', 'identifying', 'and', 'tracking', 'the', 'possible', 'carriers', 'of', 'the', 'virus.']\n",
      "['Many', 'tracing', 'apps', 'use', 'AI', 'to', 'keep', 'tabs', 'on', 'the', 'people', 'who', 'have', 'been', 'infected', 'and', 'prevent', 'the', 'risk', 'of', 'cross-infection', 'by', 'using', 'AI', 'algorithms', 'that', 'can', 'track', 'patterns', 'and', 'extract', 'some', 'features', 'to', 'classify', 'or', 'categorise', 'them.']\n",
      "['So', 'how', 'does', 'AI', 'do', 'that?']\n",
      "['IBM', 'Watson,', 'sophisticated', 'AI', 'that', 'works', 'on', 'cloud', 'computing', 'and', 'natural', 'language', 'processing,', 'has', 'prominently', 'contributed', 'to', 'the', 'healthcare', 'sector', 'on', 'global', 'level.', 'Being', 'conversational', 'AI,', 'since', '2013,', 'Watson', 'has', 'helped', 'in', 'recommending', 'treatments', 'to', 'patients', 'suffering', 'from', 'cancer', 'to', 'ensure', 'that', 'they', 'get', 'the', 'best', 'treatment', 'optimum', 'costs.']\n",
      "['Researchers', 'Google', 'Inc.', 'showed', 'that', 'an', 'AI', 'system', 'can', 'be', 'trained', 'on', 'thousands', 'of', 'images', 'to', 'achieve', 'physician-level', 'sensitivity.']\n",
      "['By', 'identifying', 'the', 'molecular', 'patterns', 'associated', 'with', 'disease', 'status', 'and', 'its', 'subtypes,', 'gene', 'expression,', 'and', 'protein', 'abundance', 'levels,', 'machine', 'learning', 'methods', 'can', 'detect', 'fatal', 'diseases', 'like', 'cancer', 'an', 'early', 'stage.', 'Machine', 'Learning', '(ML)', 'techniques', 'focus', 'mainly', 'on', 'analyzing', 'structured', 'data,', 'which', 'can', 'further', 'help', 'in', 'clustering', 'patients’', 'traits', 'and', 'infer', 'the', 'probability', 'of', 'disease', 'outcomes.', 'Since', 'patient', 'traits', 'mainly', 'include', 'masses', 'of', 'data', 'relating', 'to', 'age,', 'gender,', 'disease', 'history,', 'disease-specific', 'data', 'like', 'diagnostic', 'imaging', 'and', 'gene', 'expressions,', 'etc,', 'ML', 'can', 'extract', 'features', 'from', 'these', 'data', 'inputs', 'by', 'constructing', 'data', 'analytical', 'algorithms.']\n",
      "['ML', 'algorithms', 'are', 'either', 'supervised', 'or', 'unsupervised.', 'Unsupervised', 'learning', 'helps', 'in', 'extracting', 'features', 'and', 'clustering', 'similar', 'features', 'together', 'that', 'further', 'leads', 'to', 'early', 'detection', 'of', 'diseases.', 'Clustering', 'and', 'principal', 'component', 'analysis', 'enable', 'grouping', 'or', 'clustering', 'of', 'similar', 'traits', 'together', 'that', 'are', 'further', 'used', 'to', 'maximize', 'or', 'minimize', 'the', 'similarity', 'between', 'the', 'patients', 'within', 'or', 'between', 'the', 'clusters.', 'Since', 'patient', 'traits', 'are', 'recorded', 'in', 'multiple', 'dimensions,', 'such', 'as', 'genes,', 'principal', 'component', 'analysis(PCA)', 'creates', 'the', 'apparatus', 'to', 'reduce', 'these', 'dimensions', 'which', 'humans', 'could', 'have', 'not', 'done', 'alone.']\n",
      "['Supervised', 'learning', 'considers', 'the', 'outcomes', 'of', 'the', 'subjects', 'together', 'with', 'the', 'traits,', 'and', 'further', 'correlates', 'the', 'inputs', 'with', 'the', 'outputs', 'to', 'predict', 'the', 'probability', 'of', 'getting', 'particular', 'clinical', 'event,', 'expected', 'value', 'of', 'disease', 'level', 'or', 'expected', 'survival', 'time,', 'or', 'risk', 'of', 'Down’s', 'syndrome.']\n",
      "['Biomarker', 'panels', 'that', 'are', 'mostly', 'used', 'to', 'detect', 'ovarian', 'cancer,', 'have', 'outperformed', 'the', 'conventional', 'statistical', 'methods', 'due', 'to', 'machine', 'learning.', 'In', 'addition', 'to', 'this,', 'the', 'use', 'of', 'EHRs', 'and', 'Bayesian', 'networks,', 'which', 'are', 'part', 'of', 'supervised', 'machine', 'learning', 'algorithms,', 'can', 'predict', 'clinical', 'outcomes', 'and', 'mortality', 'respectively.']\n",
      "['Unstructured', 'data', 'such', 'as', 'clinical', 'notes', 'and', 'texts', 'are', 'converted', 'into', 'machine-readable', 'structured', 'data', 'with', 'the', 'help', 'of', 'natural', 'language', 'processing(NLP).', 'NLP', 'works', 'with', 'two', 'components:', 'text', 'processing', 'and', 'classification.', 'Text', 'processing', 'helps', 'in', 'identifying', 'series', 'of', 'disease-relevant', 'keywords', 'in', 'clinical', 'notes', 'and', 'then', 'through', 'classification', 'are', 'further', 'categorized', 'into', 'normal', 'and', 'abnormal', 'cases.', 'Chest', 'screening', 'through', 'ML', 'and', 'NLP', 'has', 'helped', 'find', 'abnormalities', 'in', 'the', 'lungs', 'and', 'provide', 'treatment', 'to', 'covid', 'patients.', 'Healthcare', 'organizations', 'use', 'NLP-based', 'chatbots', 'to', 'increase', 'interactions', 'with', 'patients,', 'keeping', 'their', 'mental', 'health', 'and', 'wellness', 'in', 'check.']\n",
      "['Deep', 'learning', 'is', 'modern', 'extension', 'of', 'the', 'classical', 'neural', 'network', 'techniques', 'which', 'helps', 'explore', 'more', 'complex', 'non-linear', 'patterns', 'in', 'data,', 'using', 'algorithms', 'like', 'convolution', 'neural', 'network,', 'recurrent', 'neural', 'network,', 'deep', 'belief', 'network,', 'and', 'deep', 'neural', 'network', 'which', 'enables', 'more', 'accurate', 'clinical', 'prediction.', 'When', 'comes', 'to', 'genome', 'interpretation,', 'deep', 'neural', 'networks', 'surpass', 'the', 'conventional', 'methods', 'of', 'logistics', 'regression', 'and', 'support', 'vector', 'machines.']\n",
      "['Sepsis', 'Watch', 'is', 'an', 'AI', 'system', 'trained', 'in', 'deep', 'learning', 'algorithms', 'that', 'holds', 'the', 'capability', 'to', 'analyze', 'over', '32', 'million', 'data', 'points', 'to', 'create', 'patient’s', 'risk', 'score', 'and', 'identify', 'the', 'early', 'stages', 'of', 'sepsis.']\n",
      "['Another', 'method', 'known', 'as', 'the', 'Learning-based', 'Optimization', 'of', 'the', 'Under', 'Sampling', 'Pattern(', 'LOUPE)', 'is', 'based', 'on', 'integrating', 'full', 'resolution', 'MRI', 'scans', 'with', 'the', 'convolutional', 'neural', 'network', 'algorithm,', 'which', 'helps', 'in', 'creating', 'more', 'accurate', 'reconstructions.']\n",
      "['Robotic', 'surgery', 'is', 'widely', 'considered', 'in', 'most', 'delicate', 'surgeries', 'like', 'gynaecology', 'and', 'prostate', 'surgery.', 'Even', 'after', 'striking', 'the', 'right', 'balance', 'between', 'human', 'decisions', 'and', 'AI', 'precision,', 'robotic', 'surgery', 'reduces', 'surgeon', 'efficiency', 'as', 'they', 'have', 'to', 'be', 'manually', 'operated', 'through', 'console.', 'Thus,', 'autonomous', 'robotic', 'surgery', 'is', 'on', 'the', 'rise', 'with', 'inventions', 'such', 'as', 'robotic', 'silicon', 'fingers', 'that', 'mimic', 'the', 'sense', 'of', 'touch', 'that', 'surgeons', 'need', 'to', 'identify', 'organs,', 'cut', 'tissues,', 'etc.,', 'or', 'robotic', 'catheters', 'that', 'can', 'navigate', 'whether', 'is', 'touching', 'blood,', 'tissue,', 'or', 'valve.']\n",
      "['Researchers', 'Children’s', 'National', 'Hospital,', 'Washington', 'have', 'already', 'developed', 'an', 'AI', 'called', 'Smart', 'Tissue', 'Autonomous', 'Robot', '(STAR),', 'which', 'performs', 'colon', 'anastomosis', 'on', 'its', 'own', 'with', 'the', 'help', 'of', 'an', 'ML-powered', 'suturing', 'tool,', 'that', 'automatically', 'detects', 'the', 'patient’s', 'breathing', 'pattern', 'to', 'apply', 'suture', 'the', 'correct', 'point.']\n",
      "['Cloud', 'computing', 'in', 'healthcare', 'has', 'helped', 'in', 'retrieving', 'and', 'sharing', 'medical', 'records', 'safely', 'with', 'reduction', 'in', 'maintenance', 'costs.', 'Through', 'this', 'technology', 'doctors', 'and', 'various', 'healthcare', 'workers', 'have', 'access', 'to', 'detailed', 'patient', 'data', 'that', 'helps', 'in', 'speeding', 'up', 'analysis', 'ultimately', 'leading', 'to', 'better', 'care', 'in', 'the', 'form', 'of', 'more', 'accurate', 'information,', 'medications,', 'and', 'therapies.']\n",
      "['How', 'can', 'help', 'in', 'Biomedical', 'research?']\n",
      "['Since', 'AI', 'can', 'analyze', 'literature', 'beyond', 'readability,', 'can', 'be', 'used', 'to', 'concise', 'biomedical', 'research.', 'With', 'the', 'help', 'of', 'ML', 'algorithms', 'and', 'NLP,', 'AI', 'can', 'accelerate', 'screening', 'and', 'indexing', 'of', 'biomedical', 'research,', 'by', 'ranking', 'the', 'literature', 'of', 'interest', 'which', 'allows', 'researchers', 'to', 'formulate', 'and', 'test', 'scientific', 'hypotheses', 'far', 'more', 'precisely', 'and', 'quickly.', 'Taking', 'to', 'the', 'next', 'level,', 'AI', 'systems', 'like', 'the', 'computational', 'modelling', 'assistant', '(CMA)', 'helps', 'researchers', 'to', 'construct', 'simulation', 'models', 'from', 'the', 'concepts', 'they', 'have', 'in', 'mind.', 'Such', 'innovations', 'have', 'majorly', 'contributed', 'to', 'topics', 'such', 'as', 'tumour', 'suppressor', 'mechanisms', 'and', 'protein-protein', 'interaction', 'information', 'extraction.']\n",
      "['AI', 'as', 'precision', 'medicine']\n",
      "['Since', 'precision', 'medicine', 'focuses', 'on', 'healthcare', 'interventions', 'to', 'individuals', 'or', 'groups', 'of', 'patients', 'based', 'on', 'their', 'profile,', 'the', 'various', 'AI', 'devices', 'pave', 'the', 'way', 'to', 'practice', 'more', 'efficiently.', 'With', 'the', 'help', 'of', 'ML,', 'complex', 'algorithms', 'like', 'large', 'datasets', 'can', 'be', 'used', 'to', 'predict', 'and', 'create', 'an', 'optimal', 'treatment', 'strategy.']\n",
      "['Deep', 'learning', 'and', 'neural', 'networks', 'can', 'be', 'used', 'to', 'process', 'data', 'in', 'healthcare', 'apps', 'and', 'keep', 'close', 'watch', 'on', 'the', 'patient’s', 'emotional', 'state,', 'food', 'intake,', 'or', 'health', 'monitoring.']\n",
      "['“Omics”', 'refers', 'to', 'the', 'collective', 'technologies', 'that', 'help', 'in', 'exploring', 'the', 'roles,', 'relationships', 'of', 'various', 'branches', 'ending', 'with', 'the', 'suffix', '“omics”', 'such', 'as', 'genomics,', 'proteomics,', 'etc.', 'Omics-based', 'tests', 'based', 'on', 'machine', 'learning', 'algorithms', 'help', 'find', 'correlations', 'and', 'predict', 'treatment', 'responses,', 'ultimately', 'creating', 'personalized', 'treatments', 'for', 'individual', 'patients.']\n",
      "['How', 'helps', 'in', 'psychology', 'and', 'neuro', 'patients']\n",
      "['For', 'psychologists', 'studying', 'creativity,', 'AI', 'is', 'promising', 'new', 'classes', 'of', 'experiments', 'that', 'are', 'developing', 'data', 'structures', 'and', 'programs', 'and', 'exploring', 'novel', 'theories', 'on', 'new', 'horizon.', 'Studies', 'show', 'that', 'AI', 'can', 'conduct', 'therapy', 'sessions,', 'e-therapy', 'sessions,', 'and', 'assessments', 'autonomously,', 'also', 'assisting', 'human', 'practitioners', 'before,', 'during,', 'or', 'after', 'sessions.', 'The', 'Detection', 'and', 'Computational', 'Analysis', 'of', 'Psychological', 'Signal', 'project', 'uses', 'ML,', 'computer', 'vision,', 'and', 'NLP', 'to', 'analyze', 'language,', 'physical', 'gestures,', 'and', 'social', 'signals', 'to', 'identify', 'cues', 'for', 'human', 'distress.', 'This', 'ground-breaking', 'technology', 'assesses', 'soldiers', 'returning', 'from', 'combat', 'and', 'recognizes', 'those', 'who', 'require', 'further', 'mental', 'health', 'support.', 'In', 'the', 'future,', 'will', 'combine', 'data', 'captured', 'during', 'face-to-face', 'interviews', 'with', 'information', 'on', 'sleeping,', 'eating,', 'and', 'online', 'behaviours', 'for', 'complete', 'patient', 'view.']\n",
      "['Stroke', 'identification']\n",
      "['Stroke', 'is', 'another', 'frequently', 'occurring', 'disease', 'that', 'affects', 'more', 'than', '500', 'million', 'people', 'worldwide.', 'Thrombus,', 'in', 'the', 'vessel', 'cerebral', 'infarction', 'is', 'the', 'major', '(about', '85%)', 'cause', 'of', 'stroke', 'occurrence.', 'In', 'recent', 'years,', 'AI', 'techniques', 'have', 'been', 'used', 'in', 'numerous', 'stroke-related', 'studies', 'as', 'early', 'detection', 'and', 'timely', 'treatment', 'along', 'with', 'efficient', 'outcome', 'prediction', 'can', 'help', 'solve', 'the', 'problem.', 'With', 'AI', 'our', 'disposal,', 'large', 'amounts', 'of', 'data', 'with', 'rich', 'information,', 'more', 'complications', 'and', 'real-life', 'clinical', 'questions', 'can', 'be', 'addressed', 'in', 'this', 'arena.', 'Currently,', 'two', 'ML', 'algorithms-', 'genetic', 'fuzzy', 'finite', 'state', 'machine', 'and', 'PCA', 'were', 'implemented', 'to', 'build', 'model', 'building', 'solution.', 'These', 'include', 'human', 'activity', 'recognition', 'stage', 'and', 'stroke', 'onset', 'detection', 'stage.', 'An', 'alert', 'stroke', 'message', 'is', 'activated', 'as', 'soon', 'as', 'movement', 'significantly', 'different', 'from', 'the', 'normal', 'pattern', 'is', 'recorded.', 'ML', 'methods', 'have', 'been', 'applied', 'to', 'neuroimaging', 'data', 'to', 'assist', 'disease', 'evaluation', 'and', 'predicting', 'stroke', 'treatment', 'for', 'the', 'diagnosis.']\n",
      "['Patient', 'Monitoring']\n",
      "['Today,', 'the', 'market', 'for', 'AI-based', 'patient', 'monitoring', 'is', 'impressive', 'and', 'monetarily', 'enticing.', 'is', 'evolving', 'with', 'artificial', 'sensors,', 'smart', 'technologies', 'and', 'explores', 'everything', 'from', 'brain-computer', 'interfaces', 'to', 'nanorobotics.', 'Companies', 'with', 'their', 'smart-watches', 'have', 'engaged', 'people', 'to', 'perform', 'remote', 'monitoring', 'even', 'when', 'they', 'are', 'not', '“patients”.', 'An', 'obvious', 'place', 'to', 'start', 'is', 'with', 'wearable', 'and', 'embedded', 'sensors,', 'glucose', 'monitors,', 'pulse', 'monitors,', 'oximeters,', 'and', 'ECG', 'monitors.', 'With', 'patient', 'monitoring', 'becoming', 'crucial,', 'AI', 'finds', 'numerous', 'applications', 'in', 'chronic', 'conditions,', 'intensive', 'care', 'units,', 'operating', 'rooms,', 'emergency', 'rooms,', 'and', 'cardiac', 'wards', 'where', 'timeless', 'clinical', 'decision-making', 'can', 'be', 'measured', 'in', 'seconds.', 'More', 'advances', 'have', 'started', 'to', 'gain', 'traction', 'like', 'smart', 'prosthetics', 'and', 'implants.', 'These', 'play', 'an', 'impeccable', 'role', 'in', 'patient', 'management', 'post-surgery', 'or', 'rehabilitation.', 'Demographics,', 'laboratory', 'results', 'and', 'vital', 'signs', 'can', 'also', 'be', 'used', 'to', 'predict', 'cardiac', 'arrest,', 'transfer', 'into', 'the', 'intensive', 'care', 'unit,', 'or', 'even', 'death.', 'In', 'addition,', 'an', 'interpretable', 'machine-learning', 'model', 'can', 'assist', 'anesthesiologists', 'in', 'predicting', 'hypoxaemia', 'events', 'during', 'surgery.', 'This', 'suggests', 'that', 'with', 'deep-learning', 'algorithms,', 'raw', 'patient-monitoring', 'data', 'could', 'be', 'better', 'used', 'to', 'avoid', 'information', 'overload', 'and', 'alert', 'overload', 'while', 'enabling', 'more', 'accurate', 'clinical', 'prediction', 'and', 'timely', 'decision-making.']\n",
      "['Conclusion']\n",
      "['Considering', 'the', 'vast', 'range', 'of', 'tasks', 'that', 'an', 'AI', 'can', 'do,', 'is', 'evident', 'that', 'holds', 'deep', 'potential', 'in', 'improving', 'patient', 'outcomes', 'to', 'skyrocketing', 'levels.', 'Using', 'sophisticated', 'algorithms', 'AI', 'can', 'bring', 'revolution', 'in', 'the', 'healthcare', 'sector.', 'Even', 'after', 'facing', 'challenges', 'like', 'whether', 'the', 'technology', 'will', 'be', 'able', 'to', 'deliver', 'the', 'promises,', 'ethical', 'measures,', 'training', 'physicians', 'to', 'use', 'it,', 'standard', 'regulations', 'etc,', 'the', 'role', 'of', 'AI', 'in', 'transforming', 'the', 'clinical', 'practices', 'cannot', 'be', 'ignored.', 'The', 'biggest', 'challenge', 'is', 'the', 'integration', 'of', 'AI', 'in', 'daily', 'practice.', 'All', 'of', 'these', 'can', 'be', 'overcome', 'and', 'within', 'that', 'period', 'the', 'technologies', 'will', 'mature', 'making', 'the', 'system', 'far', 'more', 'enhanced', 'and', 'effective.']\n",
      "['We', 'provide', 'intelligence,', 'accelerate', 'innovation', 'and', 'implement', 'technology', 'with', 'extraordinary', 'breadth', 'and', 'depth', 'global', 'insights', 'into', 'the', 'big', 'data,data-driven', 'dashboards,', 'applications', 'development,', 'and', 'information', 'management', 'for', 'organizations', 'through', 'combining', 'unique,', 'specialist', 'services', 'and', 'high-lvel', 'human', 'expertise.']\n",
      "['Contact', 'us:', 'hello@blackcoffer.com']\n",
      "['©', 'All', 'Right', 'Reserved,', 'Blackcoffer(OPC)', 'Pvt.', 'Ltd']\n",
      "['Introduction']\n",
      "['“If', 'anything', 'kills', 'over', '10', 'million', 'people', 'in', 'the', 'next', 'few', 'decades,', 'it', 'will', 'be', 'a', 'highly', 'infectious', 'virus', 'rather', 'than', 'a', 'war.', 'Not', 'missiles', 'but', 'microbes.”', 'Bill', 'Gates’s', 'remarks', 'at', 'a', 'TED', 'conference', 'in', '2014,', 'right', 'after', 'the', 'world', 'had', 'avoided', 'the', 'Ebola', 'outbreak.', 'When', 'the', 'new,', 'unprecedented,', 'invisible', 'virus', 'hit', 'us,', 'it', 'met', 'an', 'overwhelmed', 'and', 'unprepared', 'healthcare', 'system', 'and', 'oblivious', 'population.', 'This', 'public', 'health', 'emergency', 'demonstrated', 'our', 'lack', 'of', 'scientific', 'consideration', 'and', 'underlined', 'the', 'alarming', 'need', 'for', 'robust', 'innovations', 'in', 'our', 'health', 'and', 'medical', 'facilities.', 'For', 'the', 'past', 'few', 'years,', 'artificial', 'intelligence', 'has', 'proven', 'to', 'be', 'of', 'tangible', 'potential', 'in', 'the', 'healthcare', 'sectors,', 'clinical', 'practices,', 'translational', 'medical', 'and', 'biomedical', 'research.']\n",
      "['After', 'the', 'first', 'case', 'was', 'detected', 'in', 'China', 'on', 'December', '31st', '2019,', 'it', 'was', 'an', 'AI', 'program', 'developed', 'by', 'BlueDot', 'that', 'alerted', 'the', 'world', 'about', 'the', 'pandemic.', 'It', 'was', 'quick', 'to', 'realise', 'AI’s', 'ability', 'to', 'analyse', 'large', 'chunks', 'of', 'data', 'could', 'help', 'in', 'detecting', 'patterns', 'and', 'identifying', 'and', 'tracking', 'the', 'possible', 'carriers', 'of', 'the', 'virus.']\n",
      "['Many', 'tracing', 'apps', 'use', 'AI', 'to', 'keep', 'tabs', 'on', 'the', 'people', 'who', 'have', 'been', 'infected', 'and', 'prevent', 'the', 'risk', 'of', 'cross-infection', 'by', 'using', 'AI', 'algorithms', 'that', 'can', 'track', 'patterns', 'and', 'extract', 'some', 'features', 'to', 'classify', 'or', 'categorise', 'them.']\n",
      "['So', 'how', 'does', 'AI', 'do', 'that?']\n",
      "['IBM', 'Watson,', 'a', 'sophisticated', 'AI', 'that', 'works', 'on', 'cloud', 'computing', 'and', 'natural', 'language', 'processing,', 'has', 'prominently', 'contributed', 'to', 'the', 'healthcare', 'sector', 'on', 'a', 'global', 'level.', 'Being', 'a', 'conversational', 'AI,', 'since', '2013,', 'Watson', 'has', 'helped', 'in', 'recommending', 'treatments', 'to', 'patients', 'suffering', 'from', 'cancer', 'to', 'ensure', 'that', 'they', 'get', 'the', 'best', 'treatment', 'at', 'optimum', 'costs.']\n",
      "['Researchers', 'at', 'Google', 'Inc.', 'showed', 'that', 'an', 'AI', 'system', 'can', 'be', 'trained', 'on', 'thousands', 'of', 'images', 'to', 'achieve', 'physician-level', 'sensitivity.']\n",
      "['By', 'identifying', 'the', 'molecular', 'patterns', 'associated', 'with', 'disease', 'status', 'and', 'its', 'subtypes,', 'gene', 'expression,', 'and', 'protein', 'abundance', 'levels,', 'machine', 'learning', 'methods', 'can', 'detect', 'fatal', 'diseases', 'like', 'cancer', 'at', 'an', 'early', 'stage.', 'Machine', 'Learning', '(ML)', 'techniques', 'focus', 'mainly', 'on', 'analyzing', 'structured', 'data,', 'which', 'can', 'further', 'help', 'in', 'clustering', 'patients’', 'traits', 'and', 'infer', 'the', 'probability', 'of', 'disease', 'outcomes.', 'Since', 'patient', 'traits', 'mainly', 'include', 'masses', 'of', 'data', 'relating', 'to', 'age,', 'gender,', 'disease', 'history,', 'disease-specific', 'data', 'like', 'diagnostic', 'imaging', 'and', 'gene', 'expressions,', 'etc,', 'ML', 'can', 'extract', 'features', 'from', 'these', 'data', 'inputs', 'by', 'constructing', 'data', 'analytical', 'algorithms.']\n",
      "['ML', 'algorithms', 'are', 'either', 'supervised', 'or', 'unsupervised.', 'Unsupervised', 'learning', 'helps', 'in', 'extracting', 'features', 'and', 'clustering', 'similar', 'features', 'together', 'that', 'further', 'leads', 'to', 'early', 'detection', 'of', 'diseases.', 'Clustering', 'and', 'principal', 'component', 'analysis', 'enable', 'grouping', 'or', 'clustering', 'of', 'similar', 'traits', 'together', 'that', 'are', 'further', 'used', 'to', 'maximize', 'or', 'minimize', 'the', 'similarity', 'between', 'the', 'patients', 'within', 'or', 'between', 'the', 'clusters.', 'Since', 'patient', 'traits', 'are', 'recorded', 'in', 'multiple', 'dimensions,', 'such', 'as', 'genes,', 'principal', 'component', 'analysis(PCA)', 'creates', 'the', 'apparatus', 'to', 'reduce', 'these', 'dimensions', 'which', 'humans', 'could', 'have', 'not', 'done', 'alone.']\n",
      "['Supervised', 'learning', 'considers', 'the', 'outcomes', 'of', 'the', 'subjects', 'together', 'with', 'the', 'traits,', 'and', 'further', 'correlates', 'the', 'inputs', 'with', 'the', 'outputs', 'to', 'predict', 'the', 'probability', 'of', 'getting', 'a', 'particular', 'clinical', 'event,', 'expected', 'value', 'of', 'a', 'disease', 'level', 'or', 'expected', 'survival', 'time,', 'or', 'risk', 'of', 'Down’s', 'syndrome.']\n",
      "['Biomarker', 'panels', 'that', 'are', 'mostly', 'used', 'to', 'detect', 'ovarian', 'cancer,', 'have', 'outperformed', 'the', 'conventional', 'statistical', 'methods', 'due', 'to', 'machine', 'learning.', 'In', 'addition', 'to', 'this,', 'the', 'use', 'of', 'EHRs', 'and', 'Bayesian', 'networks,', 'which', 'are', 'a', 'part', 'of', 'supervised', 'machine', 'learning', 'algorithms,', 'can', 'predict', 'clinical', 'outcomes', 'and', 'mortality', 'respectively.']\n",
      "['Unstructured', 'data', 'such', 'as', 'clinical', 'notes', 'and', 'texts', 'are', 'converted', 'into', 'machine-readable', 'structured', 'data', 'with', 'the', 'help', 'of', 'natural', 'language', 'processing(NLP).', 'NLP', 'works', 'with', 'two', 'components:', 'text', 'processing', 'and', 'classification.', 'Text', 'processing', 'helps', 'in', 'identifying', 'a', 'series', 'of', 'disease-relevant', 'keywords', 'in', 'clinical', 'notes', 'and', 'then', 'through', 'classification', 'are', 'further', 'categorized', 'into', 'normal', 'and', 'abnormal', 'cases.', 'Chest', 'screening', 'through', 'ML', 'and', 'NLP', 'has', 'helped', 'find', 'abnormalities', 'in', 'the', 'lungs', 'and', 'provide', 'treatment', 'to', 'covid', 'patients.', 'Healthcare', 'organizations', 'use', 'NLP-based', 'chatbots', 'to', 'increase', 'interactions', 'with', 'patients,', 'keeping', 'their', 'mental', 'health', 'and', 'wellness', 'in', 'check.']\n",
      "['Deep', 'learning', 'is', 'a', 'modern', 'extension', 'of', 'the', 'classical', 'neural', 'network', 'techniques', 'which', 'helps', 'explore', 'more', 'complex', 'non-linear', 'patterns', 'in', 'data,', 'using', 'algorithms', 'like', 'convolution', 'neural', 'network,', 'recurrent', 'neural', 'network,', 'deep', 'belief', 'network,', 'and', 'deep', 'neural', 'network', 'which', 'enables', 'more', 'accurate', 'clinical', 'prediction.', 'When', 'it', 'comes', 'to', 'genome', 'interpretation,', 'deep', 'neural', 'networks', 'surpass', 'the', 'conventional', 'methods', 'of', 'logistics', 'regression', 'and', 'support', 'vector', 'machines.']\n",
      "['Sepsis', 'Watch', 'is', 'an', 'AI', 'system', 'trained', 'in', 'deep', 'learning', 'algorithms', 'that', 'holds', 'the', 'capability', 'to', 'analyze', 'over', '32', 'million', 'data', 'points', 'to', 'create', 'a', 'patient’s', 'risk', 'score', 'and', 'identify', 'the', 'early', 'stages', 'of', 'sepsis.']\n",
      "['Another', 'method', 'known', 'as', 'the', 'Learning-based', 'Optimization', 'of', 'the', 'Under', 'Sampling', 'Pattern(', 'LOUPE)', 'is', 'based', 'on', 'integrating', 'full', 'resolution', 'MRI', 'scans', 'with', 'the', 'convolutional', 'neural', 'network', 'algorithm,', 'which', 'helps', 'in', 'creating', 'more', 'accurate', 'reconstructions.']\n",
      "['Robotic', 'surgery', 'is', 'widely', 'considered', 'in', 'most', 'delicate', 'surgeries', 'like', 'gynaecology', 'and', 'prostate', 'surgery.', 'Even', 'after', 'striking', 'the', 'right', 'balance', 'between', 'human', 'decisions', 'and', 'AI', 'precision,', 'robotic', 'surgery', 'reduces', 'surgeon', 'efficiency', 'as', 'they', 'have', 'to', 'be', 'manually', 'operated', 'through', 'a', 'console.', 'Thus,', 'autonomous', 'robotic', 'surgery', 'is', 'on', 'the', 'rise', 'with', 'inventions', 'such', 'as', 'robotic', 'silicon', 'fingers', 'that', 'mimic', 'the', 'sense', 'of', 'touch', 'that', 'surgeons', 'need', 'to', 'identify', 'organs,', 'cut', 'tissues,', 'etc.,', 'or', 'robotic', 'catheters', 'that', 'can', 'navigate', 'whether', 'it', 'is', 'touching', 'blood,', 'tissue,', 'or', 'valve.']\n",
      "['Researchers', 'at', 'Children’s', 'National', 'Hospital,', 'Washington', 'have', 'already', 'developed', 'an', 'AI', 'called', 'Smart', 'Tissue', 'Autonomous', 'Robot', '(STAR),', 'which', 'performs', 'a', 'colon', 'anastomosis', 'on', 'its', 'own', 'with', 'the', 'help', 'of', 'an', 'ML-powered', 'suturing', 'tool,', 'that', 'automatically', 'detects', 'the', 'patient’s', 'breathing', 'pattern', 'to', 'apply', 'suture', 'at', 'the', 'correct', 'point.']\n",
      "['Cloud', 'computing', 'in', 'healthcare', 'has', 'helped', 'in', 'retrieving', 'and', 'sharing', 'medical', 'records', 'safely', 'with', 'a', 'reduction', 'in', 'maintenance', 'costs.', 'Through', 'this', 'technology', 'doctors', 'and', 'various', 'healthcare', 'workers', 'have', 'access', 'to', 'detailed', 'patient', 'data', 'that', 'helps', 'in', 'speeding', 'up', 'analysis', 'ultimately', 'leading', 'to', 'better', 'care', 'in', 'the', 'form', 'of', 'more', 'accurate', 'information,', 'medications,', 'and', 'therapies.']\n",
      "['How', 'can', 'It', 'help', 'in', 'Biomedical', 'research?']\n",
      "['Since', 'AI', 'can', 'analyze', 'literature', 'beyond', 'readability,', 'it', 'can', 'be', 'used', 'to', 'concise', 'biomedical', 'research.', 'With', 'the', 'help', 'of', 'ML', 'algorithms', 'and', 'NLP,', 'AI', 'can', 'accelerate', 'screening', 'and', 'indexing', 'of', 'biomedical', 'research,', 'by', 'ranking', 'the', 'literature', 'of', 'interest', 'which', 'allows', 'researchers', 'to', 'formulate', 'and', 'test', 'scientific', 'hypotheses', 'far', 'more', 'precisely', 'and', 'quickly.', 'Taking', 'it', 'to', 'the', 'next', 'level,', 'AI', 'systems', 'like', 'the', 'computational', 'modelling', 'assistant', '(CMA)', 'helps', 'researchers', 'to', 'construct', 'simulation', 'models', 'from', 'the', 'concepts', 'they', 'have', 'in', 'mind.', 'Such', 'innovations', 'have', 'majorly', 'contributed', 'to', 'topics', 'such', 'as', 'tumour', 'suppressor', 'mechanisms', 'and', 'protein-protein', 'interaction', 'information', 'extraction.']\n",
      "['AI', 'as', 'precision', 'medicine']\n",
      "['Since', 'precision', 'medicine', 'focuses', 'on', 'healthcare', 'interventions', 'to', 'individuals', 'or', 'groups', 'of', 'patients', 'based', 'on', 'their', 'profile,', 'the', 'various', 'AI', 'devices', 'pave', 'the', 'way', 'to', 'practice', 'it', 'more', 'efficiently.', 'With', 'the', 'help', 'of', 'ML,', 'complex', 'algorithms', 'like', 'large', 'datasets', 'can', 'be', 'used', 'to', 'predict', 'and', 'create', 'an', 'optimal', 'treatment', 'strategy.']\n",
      "['Deep', 'learning', 'and', 'neural', 'networks', 'can', 'be', 'used', 'to', 'process', 'data', 'in', 'healthcare', 'apps', 'and', 'keep', 'a', 'close', 'watch', 'on', 'the', 'patient’s', 'emotional', 'state,', 'food', 'intake,', 'or', 'health', 'monitoring.']\n",
      "['“Omics”', 'refers', 'to', 'the', 'collective', 'technologies', 'that', 'help', 'in', 'exploring', 'the', 'roles,', 'relationships', 'of', 'various', 'branches', 'ending', 'with', 'the', 'suffix', '“omics”', 'such', 'as', 'genomics,', 'proteomics,', 'etc.', 'Omics-based', 'tests', 'based', 'on', 'machine', 'learning', 'algorithms', 'help', 'find', 'correlations', 'and', 'predict', 'treatment', 'responses,', 'ultimately', 'creating', 'personalized', 'treatments', 'for', 'individual', 'patients.']\n",
      "['How', 'it', 'helps', 'in', 'psychology', 'and', 'neuro', 'patients']\n",
      "['For', 'psychologists', 'studying', 'creativity,', 'AI', 'is', 'promising', 'new', 'classes', 'of', 'experiments', 'that', 'are', 'developing', 'data', 'structures', 'and', 'programs', 'and', 'exploring', 'novel', 'theories', 'on', 'a', 'new', 'horizon.', 'Studies', 'show', 'that', 'AI', 'can', 'conduct', 'therapy', 'sessions,', 'e-therapy', 'sessions,', 'and', 'assessments', 'autonomously,', 'also', 'assisting', 'human', 'practitioners', 'before,', 'during,', 'or', 'after', 'sessions.', 'The', 'Detection', 'and', 'Computational', 'Analysis', 'of', 'Psychological', 'Signal', 'project', 'uses', 'ML,', 'computer', 'vision,', 'and', 'NLP', 'to', 'analyze', 'language,', 'physical', 'gestures,', 'and', 'social', 'signals', 'to', 'identify', 'cues', 'for', 'human', 'distress.', 'This', 'ground-breaking', 'technology', 'assesses', 'soldiers', 'returning', 'from', 'combat', 'and', 'recognizes', 'those', 'who', 'require', 'further', 'mental', 'health', 'support.', 'In', 'the', 'future,', 'it', 'will', 'combine', 'data', 'captured', 'during', 'face-to-face', 'interviews', 'with', 'information', 'on', 'sleeping,', 'eating,', 'and', 'online', 'behaviours', 'for', 'a', 'complete', 'patient', 'view.']\n",
      "['Stroke', 'identification']\n",
      "['Stroke', 'is', 'another', 'frequently', 'occurring', 'disease', 'that', 'affects', 'more', 'than', '500', 'million', 'people', 'worldwide.', 'Thrombus,', 'in', 'the', 'vessel', 'cerebral', 'infarction', 'is', 'the', 'major', '(about', '85%)', 'cause', 'of', 'stroke', 'occurrence.', 'In', 'recent', 'years,', 'AI', 'techniques', 'have', 'been', 'used', 'in', 'numerous', 'stroke-related', 'studies', 'as', 'early', 'detection', 'and', 'timely', 'treatment', 'along', 'with', 'efficient', 'outcome', 'prediction', 'can', 'help', 'solve', 'the', 'problem.', 'With', 'AI', 'at', 'our', 'disposal,', 'large', 'amounts', 'of', 'data', 'with', 'rich', 'information,', 'more', 'complications', 'and', 'real-life', 'clinical', 'questions', 'can', 'be', 'addressed', 'in', 'this', 'arena.', 'Currently,', 'two', 'ML', 'algorithms-', 'genetic', 'fuzzy', 'finite', 'state', 'machine', 'and', 'PCA', 'were', 'implemented', 'to', 'build', 'a', 'model', 'building', 'solution.', 'These', 'include', 'a', 'human', 'activity', 'recognition', 'stage', 'and', 'a', 'stroke', 'onset', 'detection', 'stage.', 'An', 'alert', 'stroke', 'message', 'is', 'activated', 'as', 'soon', 'as', 'a', 'movement', 'significantly', 'different', 'from', 'the', 'normal', 'pattern', 'is', 'recorded.', 'ML', 'methods', 'have', 'been', 'applied', 'to', 'neuroimaging', 'data', 'to', 'assist', 'disease', 'evaluation', 'and', 'predicting', 'stroke', 'treatment', 'for', 'the', 'diagnosis.']\n",
      "['Patient', 'Monitoring']\n",
      "['Today,', 'the', 'market', 'for', 'AI-based', 'patient', 'monitoring', 'is', 'impressive', 'and', 'monetarily', 'enticing.', 'It', 'is', 'evolving', 'with', 'artificial', 'sensors,', 'smart', 'technologies', 'and', 'explores', 'everything', 'from', 'brain-computer', 'interfaces', 'to', 'nanorobotics.', 'Companies', 'with', 'their', 'smart-watches', 'have', 'engaged', 'people', 'to', 'perform', 'remote', 'monitoring', 'even', 'when', 'they', 'are', 'not', '“patients”.', 'An', 'obvious', 'place', 'to', 'start', 'is', 'with', 'wearable', 'and', 'embedded', 'sensors,', 'glucose', 'monitors,', 'pulse', 'monitors,', 'oximeters,', 'and', 'ECG', 'monitors.', 'With', 'patient', 'monitoring', 'becoming', 'crucial,', 'AI', 'finds', 'numerous', 'applications', 'in', 'chronic', 'conditions,', 'intensive', 'care', 'units,', 'operating', 'rooms,', 'emergency', 'rooms,', 'and', 'cardiac', 'wards', 'where', 'timeless', 'clinical', 'decision-making', 'can', 'be', 'measured', 'in', 'seconds.', 'More', 'advances', 'have', 'started', 'to', 'gain', 'traction', 'like', 'smart', 'prosthetics', 'and', 'implants.', 'These', 'play', 'an', 'impeccable', 'role', 'in', 'patient', 'management', 'post-surgery', 'or', 'rehabilitation.', 'Demographics,', 'laboratory', 'results', 'and', 'vital', 'signs', 'can', 'also', 'be', 'used', 'to', 'predict', 'cardiac', 'arrest,', 'transfer', 'into', 'the', 'intensive', 'care', 'unit,', 'or', 'even', 'death.', 'In', 'addition,', 'an', 'interpretable', 'machine-learning', 'model', 'can', 'assist', 'anesthesiologists', 'in', 'predicting', 'hypoxaemia', 'events', 'during', 'surgery.', 'This', 'suggests', 'that', 'with', 'deep-learning', 'algorithms,', 'raw', 'patient-monitoring', 'data', 'could', 'be', 'better', 'used', 'to', 'avoid', 'information', 'overload', 'and', 'alert', 'overload', 'while', 'enabling', 'more', 'accurate', 'clinical', 'prediction', 'and', 'timely', 'decision-making.']\n",
      "['Conclusion']\n",
      "['Considering', 'the', 'vast', 'range', 'of', 'tasks', 'that', 'an', 'AI', 'can', 'do,', 'it', 'is', 'evident', 'that', 'it', 'holds', 'deep', 'potential', 'in', 'improving', 'patient', 'outcomes', 'to', 'skyrocketing', 'levels.', 'Using', 'sophisticated', 'algorithms', 'AI', 'can', 'bring', 'a', 'revolution', 'in', 'the', 'healthcare', 'sector.', 'Even', 'after', 'facing', 'challenges', 'like', 'whether', 'the', 'technology', 'will', 'be', 'able', 'to', 'deliver', 'the', 'promises,', 'ethical', 'measures,', 'training', 'physicians', 'to', 'use', 'it,', 'standard', 'regulations', 'etc,', 'the', 'role', 'of', 'AI', 'in', 'transforming', 'the', 'clinical', 'practices', 'cannot', 'be', 'ignored.', 'The', 'biggest', 'challenge', 'is', 'the', 'integration', 'of', 'AI', 'in', 'daily', 'practice.', 'All', 'of', 'these', 'can', 'be', 'overcome', 'and', 'within', 'that', 'period', 'the', 'technologies', 'will', 'mature', 'making', 'the', 'system', 'far', 'more', 'enhanced', 'and', 'effective.']\n",
      "['We', 'provide', 'intelligence,', 'accelerate', 'innovation', 'and', 'implement', 'technology', 'with', 'extraordinary', 'breadth', 'and', 'depth', 'global', 'insights', 'into', 'the', 'big', 'data,data-driven', 'dashboards,', 'applications', 'development,', 'and', 'information', 'management', 'for', 'organizations', 'through', 'combining', 'unique,', 'specialist', 'services', 'and', 'high-lvel', 'human', 'expertise.']\n",
      "['Contact', 'us:', 'hello@blackcoffer.com']\n",
      "['©', 'All', 'Right', 'Reserved,', 'Blackcoffer(OPC)', 'Pvt.', 'Ltd']\n",
      "['Introduction']\n",
      "['“If', 'anything', 'kills', 'over', '10', 'million', 'people', 'next', 'few', 'decades,', 'will', 'highly', 'infectious', 'virus', 'rather', 'than', 'war.', 'Not', 'missiles', 'but', 'microbes.”', 'Bill', 'Gates’s', 'remarks', 'TED', 'conference', '2014,', 'right', 'after', 'world', 'had', 'avoided', 'Ebola', 'outbreak.', 'When', 'new,', 'unprecedented,', 'invisible', 'virus', 'hit', 'us,', 'met', 'overwhelmed', 'unprepared', 'healthcare', 'system', 'oblivious', 'population.', 'This', 'health', 'emergency', 'demonstrated', 'our', 'lack', 'of', 'scientific', 'consideration', 'underlined', 'alarming', 'need', 'for', 'robust', 'innovations', 'our', 'health', 'medical', 'facilities.', 'For', 'past', 'few', 'years,', 'artificial', 'intelligence', 'has', 'proven', 'of', 'tangible', 'potential', 'healthcare', 'sectors,', 'clinical', 'practices,', 'translational', 'medical', 'biomedical', 'research.']\n",
      "['After', 'first', 'case', 'was', 'detected', 'China', 'December', '31st', '2019,', 'was', 'program', 'developed', 'by', 'BlueDot', 'that', 'alerted', 'world', 'about', 'pandemic.', 'was', 'quick', 'realise', 'AI’s', 'ability', 'analyse', 'large', 'chunks', 'of', 'data', 'could', 'help', 'detecting', 'patterns', 'identifying', 'tracking', 'possible', 'carriers', 'of', 'virus.']\n",
      "['Many', 'tracing', 'apps', 'use', 'keep', 'tabs', 'people', 'who', 'have', 'been', 'infected', 'prevent', 'risk', 'of', 'cross-infection', 'by', 'using', 'algorithms', 'that', 'can', 'track', 'patterns', 'extract', 'some', 'features', 'classify', 'categorise', 'them.']\n",
      "['how', 'does', 'that?']\n",
      "['IBM', 'Watson,', 'sophisticated', 'that', 'works', 'cloud', 'computing', 'natural', 'language', 'processing,', 'has', 'prominently', 'contributed', 'healthcare', 'sector', 'global', 'level.', 'Being', 'conversational', 'AI,', 'since', '2013,', 'Watson', 'has', 'helped', 'recommending', 'treatments', 'patients', 'suffering', 'from', 'cancer', 'ensure', 'that', 'they', 'get', 'best', 'treatment', 'optimum', 'costs.']\n",
      "['Researchers', 'Google', 'Inc.', 'showed', 'that', 'system', 'can', 'trained', 'thousands', 'of', 'images', 'achieve', 'physician-level', 'sensitivity.']\n",
      "['By', 'identifying', 'molecular', 'patterns', 'associated', 'with', 'disease', 'status', 'its', 'subtypes,', 'gene', 'expression,', 'protein', 'abundance', 'levels,', 'machine', 'learning', 'methods', 'can', 'detect', 'fatal', 'diseases', 'like', 'cancer', 'early', 'stage.', 'Machine', 'Learning', '(ML)', 'techniques', 'focus', 'mainly', 'analyzing', 'structured', 'data,', 'which', 'can', 'further', 'help', 'clustering', 'patients’', 'traits', 'infer', 'probability', 'of', 'disease', 'outcomes.', 'Since', 'patient', 'traits', 'mainly', 'include', 'masses', 'of', 'data', 'relating', 'age,', 'gender,', 'disease', 'history,', 'disease-specific', 'data', 'like', 'diagnostic', 'imaging', 'gene', 'expressions,', 'etc,', 'ML', 'can', 'extract', 'features', 'from', 'these', 'data', 'inputs', 'by', 'constructing', 'data', 'analytical', 'algorithms.']\n",
      "['ML', 'algorithms', 'are', 'either', 'supervised', 'unsupervised.', 'Unsupervised', 'learning', 'helps', 'extracting', 'features', 'clustering', 'similar', 'features', 'together', 'that', 'further', 'leads', 'early', 'detection', 'of', 'diseases.', 'Clustering', 'principal', 'component', 'analysis', 'enable', 'grouping', 'clustering', 'of', 'similar', 'traits', 'together', 'that', 'are', 'further', 'used', 'maximize', 'minimize', 'similarity', 'between', 'patients', 'within', 'between', 'clusters.', 'Since', 'patient', 'traits', 'are', 'recorded', 'multiple', 'dimensions,', 'such', 'genes,', 'principal', 'component', 'analysis(PCA)', 'creates', 'apparatus', 'reduce', 'these', 'dimensions', 'which', 'humans', 'could', 'have', 'not', 'alone.']\n",
      "['Supervised', 'learning', 'considers', 'outcomes', 'of', 'subjects', 'together', 'with', 'traits,', 'further', 'correlates', 'inputs', 'with', 'outputs', 'predict', 'probability', 'of', 'getting', 'particular', 'clinical', 'event,', 'expected', 'value', 'of', 'disease', 'level', 'expected', 'survival', 'time,', 'risk', 'of', 'Down’s', 'syndrome.']\n",
      "['Biomarker', 'panels', 'that', 'are', 'mostly', 'used', 'detect', 'ovarian', 'cancer,', 'have', 'outperformed', 'conventional', 'statistical', 'methods', 'due', 'machine', 'learning.', 'addition', 'this,', 'use', 'of', 'EHRs', 'Bayesian', 'networks,', 'which', 'are', 'part', 'of', 'supervised', 'machine', 'learning', 'algorithms,', 'can', 'predict', 'clinical', 'outcomes', 'mortality', 'respectively.']\n",
      "['Unstructured', 'data', 'such', 'clinical', 'notes', 'texts', 'are', 'converted', 'into', 'machine-readable', 'structured', 'data', 'with', 'help', 'of', 'natural', 'language', 'processing(NLP).', 'NLP', 'works', 'with', 'two', 'components:', 'text', 'processing', 'classification.', 'Text', 'processing', 'helps', 'identifying', 'series', 'of', 'disease-relevant', 'keywords', 'clinical', 'notes', 'then', 'through', 'classification', 'are', 'further', 'categorized', 'into', 'normal', 'abnormal', 'cases.', 'Chest', 'screening', 'through', 'ML', 'NLP', 'has', 'helped', 'find', 'abnormalities', 'lungs', 'provide', 'treatment', 'covid', 'patients.', 'Healthcare', 'organizations', 'use', 'NLP-based', 'chatbots', 'increase', 'interactions', 'with', 'patients,', 'keeping', 'their', 'mental', 'health', 'wellness', 'check.']\n",
      "['Deep', 'learning', 'modern', 'extension', 'of', 'classical', 'neural', 'network', 'techniques', 'which', 'helps', 'explore', 'more', 'complex', 'non-linear', 'patterns', 'data,', 'using', 'algorithms', 'like', 'convolution', 'neural', 'network,', 'recurrent', 'neural', 'network,', 'deep', 'belief', 'network,', 'deep', 'neural', 'network', 'which', 'enables', 'more', 'accurate', 'clinical', 'prediction.', 'When', 'comes', 'genome', 'interpretation,', 'deep', 'neural', 'networks', 'surpass', 'conventional', 'methods', 'of', 'logistics', 'regression', 'support', 'vector', 'machines.']\n",
      "['Sepsis', 'Watch', 'system', 'trained', 'deep', 'learning', 'algorithms', 'that', 'holds', 'capability', 'analyze', 'over', '32', 'million', 'data', 'points', 'create', 'patient’s', 'risk', 'score', 'identify', 'early', 'stages', 'of', 'sepsis.']\n",
      "['Another', 'method', 'known', 'Learning-based', 'Optimization', 'of', 'Under', 'Sampling', 'Pattern(', 'LOUPE)', 'based', 'integrating', 'full', 'resolution', 'MRI', 'scans', 'with', 'convolutional', 'neural', 'network', 'algorithm,', 'which', 'helps', 'creating', 'more', 'accurate', 'reconstructions.']\n",
      "['Robotic', 'surgery', 'widely', 'considered', 'most', 'delicate', 'surgeries', 'like', 'gynaecology', 'prostate', 'surgery.', 'Even', 'after', 'striking', 'right', 'balance', 'between', 'human', 'decisions', 'precision,', 'robotic', 'surgery', 'reduces', 'surgeon', 'efficiency', 'they', 'have', 'manually', 'operated', 'through', 'console.', 'Thus,', 'autonomous', 'robotic', 'surgery', 'rise', 'with', 'inventions', 'such', 'robotic', 'silicon', 'fingers', 'that', 'mimic', 'sense', 'of', 'touch', 'that', 'surgeons', 'need', 'identify', 'organs,', 'cut', 'tissues,', 'etc.,', 'robotic', 'catheters', 'that', 'can', 'navigate', 'whether', 'touching', 'blood,', 'tissue,', 'valve.']\n",
      "['Researchers', 'Children’s', 'Hospital,', 'Washington', 'have', 'already', 'developed', 'called', 'Smart', 'Tissue', 'Autonomous', 'Robot', '(STAR),', 'which', 'performs', 'colon', 'anastomosis', 'its', 'own', 'with', 'help', 'of', 'ML-powered', 'suturing', 'tool,', 'that', 'automatically', 'detects', 'patient’s', 'breathing', 'pattern', 'apply', 'suture', 'correct', 'point.']\n",
      "['Cloud', 'computing', 'healthcare', 'has', 'helped', 'retrieving', 'sharing', 'medical', 'records', 'safely', 'with', 'reduction', 'maintenance', 'costs.', 'Through', 'this', 'technology', 'doctors', 'various', 'healthcare', 'workers', 'have', 'access', 'detailed', 'patient', 'data', 'that', 'helps', 'speeding', 'up', 'analysis', 'ultimately', 'leading', 'better', 'care', 'form', 'of', 'more', 'accurate', 'information,', 'medications,', 'therapies.']\n",
      "['How', 'can', 'help', 'Biomedical', 'research?']\n",
      "['Since', 'can', 'analyze', 'literature', 'beyond', 'readability,', 'can', 'used', 'concise', 'biomedical', 'research.', 'With', 'help', 'of', 'ML', 'algorithms', 'NLP,', 'can', 'accelerate', 'screening', 'indexing', 'of', 'biomedical', 'research,', 'by', 'ranking', 'literature', 'of', 'interest', 'which', 'allows', 'researchers', 'formulate', 'test', 'scientific', 'hypotheses', 'far', 'more', 'precisely', 'quickly.', 'Taking', 'next', 'level,', 'systems', 'like', 'computational', 'modelling', 'assistant', '(CMA)', 'helps', 'researchers', 'construct', 'simulation', 'models', 'from', 'concepts', 'they', 'have', 'mind.', 'Such', 'innovations', 'have', 'majorly', 'contributed', 'topics', 'such', 'tumour', 'suppressor', 'mechanisms', 'protein-protein', 'interaction', 'information', 'extraction.']\n",
      "['precision', 'medicine']\n",
      "['Since', 'precision', 'medicine', 'focuses', 'healthcare', 'interventions', 'individuals', 'groups', 'of', 'patients', 'based', 'their', 'profile,', 'various', 'devices', 'pave', 'way', 'practice', 'more', 'efficiently.', 'With', 'help', 'of', 'ML,', 'complex', 'algorithms', 'like', 'large', 'datasets', 'can', 'used', 'predict', 'create', 'optimal', 'treatment', 'strategy.']\n",
      "['Deep', 'learning', 'neural', 'networks', 'can', 'used', 'process', 'data', 'healthcare', 'apps', 'keep', 'close', 'watch', 'patient’s', 'emotional', 'state,', 'food', 'intake,', 'health', 'monitoring.']\n",
      "['“Omics”', 'refers', 'collective', 'technologies', 'that', 'help', 'exploring', 'roles,', 'relationships', 'of', 'various', 'branches', 'ending', 'with', 'suffix', '“omics”', 'such', 'genomics,', 'proteomics,', 'etc.', 'Omics-based', 'tests', 'based', 'machine', 'learning', 'algorithms', 'help', 'find', 'correlations', 'predict', 'treatment', 'responses,', 'ultimately', 'creating', 'personalized', 'treatments', 'for', 'individual', 'patients.']\n",
      "['How', 'helps', 'psychology', 'neuro', 'patients']\n",
      "['For', 'psychologists', 'studying', 'creativity,', 'promising', 'new', 'classes', 'of', 'experiments', 'that', 'are', 'developing', 'data', 'structures', 'programs', 'exploring', 'novel', 'theories', 'new', 'horizon.', 'Studies', 'show', 'that', 'can', 'conduct', 'therapy', 'sessions,', 'e-therapy', 'sessions,', 'assessments', 'autonomously,', 'also', 'assisting', 'human', 'practitioners', 'before,', 'during,', 'after', 'sessions.', 'Detection', 'Computational', 'Analysis', 'of', 'Psychological', 'Signal', 'project', 'uses', 'ML,', 'computer', 'vision,', 'NLP', 'analyze', 'language,', 'physical', 'gestures,', 'social', 'signals', 'identify', 'cues', 'for', 'human', 'distress.', 'This', 'ground-breaking', 'technology', 'assesses', 'soldiers', 'returning', 'from', 'combat', 'recognizes', 'those', 'who', 'require', 'further', 'mental', 'health', 'support.', 'future,', 'will', 'combine', 'data', 'captured', 'during', 'face-to-face', 'interviews', 'with', 'information', 'sleeping,', 'eating,', 'online', 'behaviours', 'for', 'complete', 'patient', 'view.']\n",
      "['Stroke', 'identification']\n",
      "['Stroke', 'another', 'frequently', 'occurring', 'disease', 'that', 'affects', 'more', 'than', '500', 'million', 'people', 'worldwide.', 'Thrombus,', 'vessel', 'cerebral', 'infarction', 'major', '(about', '85%)', 'cause', 'of', 'stroke', 'occurrence.', 'recent', 'years,', 'techniques', 'have', 'been', 'used', 'numerous', 'stroke-related', 'studies', 'early', 'detection', 'timely', 'treatment', 'along', 'with', 'efficient', 'outcome', 'prediction', 'can', 'help', 'solve', 'problem.', 'With', 'our', 'disposal,', 'large', 'amounts', 'of', 'data', 'with', 'rich', 'information,', 'more', 'complications', 'real-life', 'clinical', 'questions', 'can', 'addressed', 'this', 'arena.', 'Currently,', 'two', 'ML', 'algorithms-', 'genetic', 'fuzzy', 'finite', 'state', 'machine', 'PCA', 'were', 'implemented', 'build', 'model', 'building', 'solution.', 'These', 'include', 'human', 'activity', 'recognition', 'stage', 'stroke', 'onset', 'detection', 'stage.', 'alert', 'stroke', 'message', 'activated', 'soon', 'movement', 'significantly', 'different', 'from', 'normal', 'pattern', 'recorded.', 'ML', 'methods', 'have', 'been', 'applied', 'neuroimaging', 'data', 'assist', 'disease', 'evaluation', 'predicting', 'stroke', 'treatment', 'for', 'diagnosis.']\n",
      "['Patient', 'Monitoring']\n",
      "['Today,', 'market', 'for', 'AI-based', 'patient', 'monitoring', 'impressive', 'monetarily', 'enticing.', 'evolving', 'with', 'artificial', 'sensors,', 'smart', 'technologies', 'explores', 'everything', 'from', 'brain-computer', 'interfaces', 'nanorobotics.', 'Companies', 'with', 'their', 'smart-watches', 'have', 'engaged', 'people', 'perform', 'remote', 'monitoring', 'even', 'when', 'they', 'are', 'not', '“patients”.', 'obvious', 'place', 'start', 'with', 'wearable', 'embedded', 'sensors,', 'glucose', 'monitors,', 'pulse', 'monitors,', 'oximeters,', 'ECG', 'monitors.', 'With', 'patient', 'monitoring', 'becoming', 'crucial,', 'finds', 'numerous', 'applications', 'chronic', 'conditions,', 'intensive', 'care', 'units,', 'operating', 'rooms,', 'emergency', 'rooms,', 'cardiac', 'wards', 'where', 'timeless', 'clinical', 'decision-making', 'can', 'measured', 'seconds.', 'More', 'advances', 'have', 'started', 'gain', 'traction', 'like', 'smart', 'prosthetics', 'implants.', 'These', 'play', 'impeccable', 'role', 'patient', 'management', 'post-surgery', 'rehabilitation.', 'Demographics,', 'laboratory', 'results', 'vital', 'signs', 'can', 'also', 'used', 'predict', 'cardiac', 'arrest,', 'transfer', 'into', 'intensive', 'care', 'unit,', 'even', 'death.', 'addition,', 'interpretable', 'machine-learning', 'model', 'can', 'assist', 'anesthesiologists', 'predicting', 'hypoxaemia', 'events', 'during', 'surgery.', 'This', 'suggests', 'that', 'with', 'deep-learning', 'algorithms,', 'raw', 'patient-monitoring', 'data', 'could', 'better', 'used', 'avoid', 'information', 'overload', 'alert', 'overload', 'while', 'enabling', 'more', 'accurate', 'clinical', 'prediction', 'timely', 'decision-making.']\n",
      "['Conclusion']\n",
      "['Considering', 'vast', 'range', 'of', 'tasks', 'that', 'can', 'do,', 'evident', 'that', 'holds', 'deep', 'potential', 'improving', 'patient', 'outcomes', 'skyrocketing', 'levels.', 'Using', 'sophisticated', 'algorithms', 'can', 'bring', 'revolution', 'healthcare', 'sector.', 'Even', 'after', 'facing', 'challenges', 'like', 'whether', 'technology', 'will', 'able', 'deliver', 'promises,', 'ethical', 'measures,', 'training', 'physicians', 'use', 'it,', 'standard', 'regulations', 'etc,', 'role', 'of', 'transforming', 'clinical', 'practices', 'cannot', 'ignored.', 'biggest', 'challenge', 'integration', 'of', 'daily', 'practice.', 'All', 'of', 'these', 'can', 'overcome', 'within', 'that', 'period', 'technologies', 'will', 'mature', 'making', 'system', 'far', 'more', 'enhanced', 'effective.']\n",
      "['provide', 'intelligence,', 'accelerate', 'innovation', 'implement', 'technology', 'with', 'extraordinary', 'breadth', 'depth', 'global', 'insights', 'into', 'big', 'data,data-driven', 'dashboards,', 'applications', 'development,', 'information', 'management', 'for', 'organizations', 'through', 'combining', 'unique,', 'specialist', 'services', 'high-lvel', 'human', 'expertise.']\n",
      "['Contact', 'us:', 'hello@blackcoffer.com']\n",
      "['©', 'All', 'Right', 'Reserved,', 'Blackcoffer(OPC)', 'Pvt.', 'Ltd']\n",
      "['Introduction']\n",
      "['“If', 'anything', 'kills', 'over', '10', 'million', 'people', 'the', 'next', 'few', 'decades,', 'it', 'will', 'highly', 'infectious', 'virus', 'rather', 'than', 'war.', 'Not', 'missiles', 'but', 'microbes.”', 'Bill', 'Gates’s', 'remarks', 'conference', '2014,', 'right', 'after', 'the', 'world', 'had', 'avoided', 'the', 'Ebola', 'outbreak.', 'When', 'the', 'new,', 'unprecedented,', 'invisible', 'virus', 'hit', 'us,', 'it', 'met', 'overwhelmed', 'and', 'unprepared', 'healthcare', 'system', 'and', 'oblivious', 'population.', 'This', 'public', 'health', 'emergency', 'demonstrated', 'our', 'lack', 'of', 'scientific', 'consideration', 'and', 'underlined', 'the', 'alarming', 'need', 'for', 'robust', 'innovations', 'our', 'health', 'and', 'medical', 'facilities.', 'For', 'the', 'past', 'few', 'years,', 'artificial', 'intelligence', 'has', 'proven', 'to', 'of', 'tangible', 'potential', 'the', 'healthcare', 'sectors,', 'clinical', 'practices,', 'translational', 'medical', 'and', 'biomedical', 'research.']\n",
      "['After', 'the', 'first', 'case', 'was', 'detected', 'China', 'December', '31st', '2019,', 'it', 'was', 'AI', 'program', 'developed', 'by', 'BlueDot', 'that', 'alerted', 'the', 'world', 'about', 'the', 'pandemic.', 'It', 'was', 'quick', 'to', 'realise', 'AI’s', 'ability', 'to', 'analyse', 'large', 'chunks', 'of', 'data', 'could', 'help', 'detecting', 'patterns', 'and', 'identifying', 'and', 'tracking', 'the', 'possible', 'carriers', 'of', 'the', 'virus.']\n",
      "['Many', 'tracing', 'apps', 'use', 'AI', 'to', 'keep', 'tabs', 'the', 'people', 'who', 'have', 'been', 'infected', 'and', 'prevent', 'the', 'risk', 'of', 'cross-infection', 'by', 'using', 'AI', 'algorithms', 'that', 'can', 'track', 'patterns', 'and', 'extract', 'some', 'features', 'to', 'classify', 'or', 'categorise', 'them.']\n",
      "['So', 'how', 'does', 'AI', 'do', 'that?']\n",
      "['IBM', 'Watson,', 'sophisticated', 'AI', 'that', 'works', 'cloud', 'computing', 'and', 'natural', 'language', 'processing,', 'has', 'prominently', 'contributed', 'to', 'the', 'healthcare', 'sector', 'global', 'level.', 'Being', 'conversational', 'AI,', 'since', '2013,', 'Watson', 'has', 'helped', 'recommending', 'treatments', 'to', 'patients', 'suffering', 'from', 'cancer', 'to', 'ensure', 'that', 'they', 'get', 'the', 'best', 'treatment', 'optimum', 'costs.']\n",
      "['Researchers', 'Google', 'Inc.', 'showed', 'that', 'AI', 'system', 'can', 'trained', 'thousands', 'of', 'images', 'to', 'achieve', 'physician-level', 'sensitivity.']\n",
      "['By', 'identifying', 'the', 'molecular', 'patterns', 'associated', 'with', 'disease', 'status', 'and', 'its', 'subtypes,', 'gene', 'expression,', 'and', 'protein', 'abundance', 'levels,', 'machine', 'learning', 'methods', 'can', 'detect', 'fatal', 'diseases', 'like', 'cancer', 'early', 'stage.', 'Machine', 'Learning', '(ML)', 'techniques', 'focus', 'mainly', 'analyzing', 'structured', 'data,', 'which', 'can', 'further', 'help', 'clustering', 'patients’', 'traits', 'and', 'infer', 'the', 'probability', 'of', 'disease', 'outcomes.', 'Since', 'patient', 'traits', 'mainly', 'include', 'masses', 'of', 'data', 'relating', 'to', 'age,', 'gender,', 'disease', 'history,', 'disease-specific', 'data', 'like', 'diagnostic', 'imaging', 'and', 'gene', 'expressions,', 'etc,', 'ML', 'can', 'extract', 'features', 'from', 'these', 'data', 'inputs', 'by', 'constructing', 'data', 'analytical', 'algorithms.']\n",
      "['ML', 'algorithms', 'are', 'either', 'supervised', 'or', 'unsupervised.', 'Unsupervised', 'learning', 'helps', 'extracting', 'features', 'and', 'clustering', 'similar', 'features', 'together', 'that', 'further', 'leads', 'to', 'early', 'detection', 'of', 'diseases.', 'Clustering', 'and', 'principal', 'component', 'analysis', 'enable', 'grouping', 'or', 'clustering', 'of', 'similar', 'traits', 'together', 'that', 'are', 'further', 'used', 'to', 'maximize', 'or', 'minimize', 'the', 'similarity', 'between', 'the', 'patients', 'within', 'or', 'between', 'the', 'clusters.', 'Since', 'patient', 'traits', 'are', 'recorded', 'multiple', 'dimensions,', 'such', 'as', 'genes,', 'principal', 'component', 'analysis(PCA)', 'creates', 'the', 'apparatus', 'to', 'reduce', 'these', 'dimensions', 'which', 'humans', 'could', 'have', 'not', 'done', 'alone.']\n",
      "['Supervised', 'learning', 'considers', 'the', 'outcomes', 'of', 'the', 'subjects', 'together', 'with', 'the', 'traits,', 'and', 'further', 'correlates', 'the', 'inputs', 'with', 'the', 'outputs', 'to', 'predict', 'the', 'probability', 'of', 'getting', 'particular', 'clinical', 'event,', 'expected', 'value', 'of', 'disease', 'level', 'or', 'expected', 'survival', 'time,', 'or', 'risk', 'of', 'Down’s', 'syndrome.']\n",
      "['Biomarker', 'panels', 'that', 'are', 'mostly', 'used', 'to', 'detect', 'ovarian', 'cancer,', 'have', 'outperformed', 'the', 'conventional', 'statistical', 'methods', 'due', 'to', 'machine', 'learning.', 'addition', 'to', 'this,', 'the', 'use', 'of', 'EHRs', 'and', 'Bayesian', 'networks,', 'which', 'are', 'part', 'of', 'supervised', 'machine', 'learning', 'algorithms,', 'can', 'predict', 'clinical', 'outcomes', 'and', 'mortality', 'respectively.']\n",
      "['Unstructured', 'data', 'such', 'as', 'clinical', 'notes', 'and', 'texts', 'are', 'converted', 'into', 'machine-readable', 'structured', 'data', 'with', 'the', 'help', 'of', 'natural', 'language', 'processing(NLP).', 'NLP', 'works', 'with', 'two', 'components:', 'text', 'processing', 'and', 'classification.', 'Text', 'processing', 'helps', 'identifying', 'series', 'of', 'disease-relevant', 'keywords', 'clinical', 'notes', 'and', 'then', 'through', 'classification', 'are', 'further', 'categorized', 'into', 'normal', 'and', 'abnormal', 'cases.', 'Chest', 'screening', 'through', 'ML', 'and', 'NLP', 'has', 'helped', 'find', 'abnormalities', 'the', 'lungs', 'and', 'provide', 'treatment', 'to', 'covid', 'patients.', 'Healthcare', 'organizations', 'use', 'NLP-based', 'chatbots', 'to', 'increase', 'interactions', 'with', 'patients,', 'keeping', 'their', 'mental', 'health', 'and', 'wellness', 'check.']\n",
      "['Deep', 'learning', 'is', 'modern', 'extension', 'of', 'the', 'classical', 'neural', 'network', 'techniques', 'which', 'helps', 'explore', 'more', 'complex', 'non-linear', 'patterns', 'data,', 'using', 'algorithms', 'like', 'convolution', 'neural', 'network,', 'recurrent', 'neural', 'network,', 'deep', 'belief', 'network,', 'and', 'deep', 'neural', 'network', 'which', 'enables', 'more', 'accurate', 'clinical', 'prediction.', 'When', 'it', 'comes', 'to', 'genome', 'interpretation,', 'deep', 'neural', 'networks', 'surpass', 'the', 'conventional', 'methods', 'of', 'logistics', 'regression', 'and', 'support', 'vector', 'machines.']\n",
      "['Sepsis', 'Watch', 'is', 'AI', 'system', 'trained', 'deep', 'learning', 'algorithms', 'that', 'holds', 'the', 'capability', 'to', 'analyze', 'over', '32', 'million', 'data', 'points', 'to', 'create', 'patient’s', 'risk', 'score', 'and', 'identify', 'the', 'early', 'stages', 'of', 'sepsis.']\n",
      "['Another', 'method', 'known', 'as', 'the', 'Learning-based', 'Optimization', 'of', 'the', 'Under', 'Sampling', 'Pattern(', 'LOUPE)', 'is', 'based', 'integrating', 'full', 'resolution', 'MRI', 'scans', 'with', 'the', 'convolutional', 'neural', 'network', 'algorithm,', 'which', 'helps', 'creating', 'more', 'accurate', 'reconstructions.']\n",
      "['Robotic', 'surgery', 'is', 'widely', 'considered', 'most', 'delicate', 'surgeries', 'like', 'gynaecology', 'and', 'prostate', 'surgery.', 'Even', 'after', 'striking', 'the', 'right', 'balance', 'between', 'human', 'decisions', 'and', 'AI', 'precision,', 'robotic', 'surgery', 'reduces', 'surgeon', 'efficiency', 'as', 'they', 'have', 'to', 'manually', 'operated', 'through', 'console.', 'Thus,', 'autonomous', 'robotic', 'surgery', 'is', 'the', 'rise', 'with', 'inventions', 'such', 'as', 'robotic', 'silicon', 'fingers', 'that', 'mimic', 'the', 'sense', 'of', 'touch', 'that', 'surgeons', 'need', 'to', 'identify', 'organs,', 'cut', 'tissues,', 'etc.,', 'or', 'robotic', 'catheters', 'that', 'can', 'navigate', 'whether', 'it', 'is', 'touching', 'blood,', 'tissue,', 'or', 'valve.']\n",
      "['Researchers', 'Children’s', 'National', 'Hospital,', 'Washington', 'have', 'already', 'developed', 'AI', 'called', 'Smart', 'Tissue', 'Autonomous', 'Robot', '(STAR),', 'which', 'performs', 'colon', 'anastomosis', 'its', 'own', 'with', 'the', 'help', 'of', 'ML-powered', 'suturing', 'tool,', 'that', 'automatically', 'detects', 'the', 'patient’s', 'breathing', 'pattern', 'to', 'apply', 'suture', 'the', 'correct', 'point.']\n",
      "['Cloud', 'computing', 'healthcare', 'has', 'helped', 'retrieving', 'and', 'sharing', 'medical', 'records', 'safely', 'with', 'reduction', 'maintenance', 'costs.', 'Through', 'this', 'technology', 'doctors', 'and', 'various', 'healthcare', 'workers', 'have', 'access', 'to', 'detailed', 'patient', 'data', 'that', 'helps', 'speeding', 'up', 'analysis', 'ultimately', 'leading', 'to', 'better', 'care', 'the', 'form', 'of', 'more', 'accurate', 'information,', 'medications,', 'and', 'therapies.']\n",
      "['How', 'can', 'It', 'help', 'Biomedical', 'research?']\n",
      "['Since', 'AI', 'can', 'analyze', 'literature', 'beyond', 'readability,', 'it', 'can', 'used', 'to', 'concise', 'biomedical', 'research.', 'With', 'the', 'help', 'of', 'ML', 'algorithms', 'and', 'NLP,', 'AI', 'can', 'accelerate', 'screening', 'and', 'indexing', 'of', 'biomedical', 'research,', 'by', 'ranking', 'the', 'literature', 'of', 'interest', 'which', 'allows', 'researchers', 'to', 'formulate', 'and', 'test', 'scientific', 'hypotheses', 'far', 'more', 'precisely', 'and', 'quickly.', 'Taking', 'it', 'to', 'the', 'next', 'level,', 'AI', 'systems', 'like', 'the', 'computational', 'modelling', 'assistant', '(CMA)', 'helps', 'researchers', 'to', 'construct', 'simulation', 'models', 'from', 'the', 'concepts', 'they', 'have', 'mind.', 'Such', 'innovations', 'have', 'majorly', 'contributed', 'to', 'topics', 'such', 'as', 'tumour', 'suppressor', 'mechanisms', 'and', 'protein-protein', 'interaction', 'information', 'extraction.']\n",
      "['AI', 'as', 'precision', 'medicine']\n",
      "['Since', 'precision', 'medicine', 'focuses', 'healthcare', 'interventions', 'to', 'individuals', 'or', 'groups', 'of', 'patients', 'based', 'their', 'profile,', 'the', 'various', 'AI', 'devices', 'pave', 'the', 'way', 'to', 'practice', 'it', 'more', 'efficiently.', 'With', 'the', 'help', 'of', 'ML,', 'complex', 'algorithms', 'like', 'large', 'datasets', 'can', 'used', 'to', 'predict', 'and', 'create', 'optimal', 'treatment', 'strategy.']\n",
      "['Deep', 'learning', 'and', 'neural', 'networks', 'can', 'used', 'to', 'process', 'data', 'healthcare', 'apps', 'and', 'keep', 'close', 'watch', 'the', 'patient’s', 'emotional', 'state,', 'food', 'intake,', 'or', 'health', 'monitoring.']\n",
      "['“Omics”', 'refers', 'to', 'the', 'collective', 'technologies', 'that', 'help', 'exploring', 'the', 'roles,', 'relationships', 'of', 'various', 'branches', 'ending', 'with', 'the', 'suffix', '“omics”', 'such', 'as', 'genomics,', 'proteomics,', 'etc.', 'Omics-based', 'tests', 'based', 'machine', 'learning', 'algorithms', 'help', 'find', 'correlations', 'and', 'predict', 'treatment', 'responses,', 'ultimately', 'creating', 'personalized', 'treatments', 'for', 'individual', 'patients.']\n",
      "['How', 'it', 'helps', 'psychology', 'and', 'neuro', 'patients']\n",
      "['For', 'psychologists', 'studying', 'creativity,', 'AI', 'is', 'promising', 'new', 'classes', 'of', 'experiments', 'that', 'are', 'developing', 'data', 'structures', 'and', 'programs', 'and', 'exploring', 'novel', 'theories', 'new', 'horizon.', 'Studies', 'show', 'that', 'AI', 'can', 'conduct', 'therapy', 'sessions,', 'e-therapy', 'sessions,', 'and', 'assessments', 'autonomously,', 'also', 'assisting', 'human', 'practitioners', 'before,', 'during,', 'or', 'after', 'sessions.', 'The', 'Detection', 'and', 'Computational', 'Analysis', 'of', 'Psychological', 'Signal', 'project', 'uses', 'ML,', 'computer', 'vision,', 'and', 'NLP', 'to', 'analyze', 'language,', 'physical', 'gestures,', 'and', 'social', 'signals', 'to', 'identify', 'cues', 'for', 'human', 'distress.', 'This', 'ground-breaking', 'technology', 'assesses', 'soldiers', 'returning', 'from', 'combat', 'and', 'recognizes', 'those', 'who', 'require', 'further', 'mental', 'health', 'support.', 'the', 'future,', 'it', 'will', 'combine', 'data', 'captured', 'during', 'face-to-face', 'interviews', 'with', 'information', 'sleeping,', 'eating,', 'and', 'online', 'behaviours', 'for', 'complete', 'patient', 'view.']\n",
      "['Stroke', 'identification']\n",
      "['Stroke', 'is', 'another', 'frequently', 'occurring', 'disease', 'that', 'affects', 'more', 'than', '500', 'million', 'people', 'worldwide.', 'Thrombus,', 'the', 'vessel', 'cerebral', 'infarction', 'is', 'the', 'major', '(about', '85%)', 'cause', 'of', 'stroke', 'occurrence.', 'recent', 'years,', 'AI', 'techniques', 'have', 'been', 'used', 'numerous', 'stroke-related', 'studies', 'as', 'early', 'detection', 'and', 'timely', 'treatment', 'along', 'with', 'efficient', 'outcome', 'prediction', 'can', 'help', 'solve', 'the', 'problem.', 'With', 'AI', 'our', 'disposal,', 'large', 'amounts', 'of', 'data', 'with', 'rich', 'information,', 'more', 'complications', 'and', 'real-life', 'clinical', 'questions', 'can', 'addressed', 'this', 'arena.', 'Currently,', 'two', 'ML', 'algorithms-', 'genetic', 'fuzzy', 'finite', 'state', 'machine', 'and', 'PCA', 'were', 'implemented', 'to', 'build', 'model', 'building', 'solution.', 'These', 'include', 'human', 'activity', 'recognition', 'stage', 'and', 'stroke', 'onset', 'detection', 'stage.', 'alert', 'stroke', 'message', 'is', 'activated', 'as', 'soon', 'as', 'movement', 'significantly', 'different', 'from', 'the', 'normal', 'pattern', 'is', 'recorded.', 'ML', 'methods', 'have', 'been', 'applied', 'to', 'neuroimaging', 'data', 'to', 'assist', 'disease', 'evaluation', 'and', 'predicting', 'stroke', 'treatment', 'for', 'the', 'diagnosis.']\n",
      "['Patient', 'Monitoring']\n",
      "['Today,', 'the', 'market', 'for', 'AI-based', 'patient', 'monitoring', 'is', 'impressive', 'and', 'monetarily', 'enticing.', 'It', 'is', 'evolving', 'with', 'artificial', 'sensors,', 'smart', 'technologies', 'and', 'explores', 'everything', 'from', 'brain-computer', 'interfaces', 'to', 'nanorobotics.', 'Companies', 'with', 'their', 'smart-watches', 'have', 'engaged', 'people', 'to', 'perform', 'remote', 'monitoring', 'even', 'when', 'they', 'are', 'not', '“patients”.', 'obvious', 'place', 'to', 'start', 'is', 'with', 'wearable', 'and', 'embedded', 'sensors,', 'glucose', 'monitors,', 'pulse', 'monitors,', 'oximeters,', 'and', 'ECG', 'monitors.', 'With', 'patient', 'monitoring', 'becoming', 'crucial,', 'AI', 'finds', 'numerous', 'applications', 'chronic', 'conditions,', 'intensive', 'care', 'units,', 'operating', 'rooms,', 'emergency', 'rooms,', 'and', 'cardiac', 'wards', 'where', 'timeless', 'clinical', 'decision-making', 'can', 'measured', 'seconds.', 'More', 'advances', 'have', 'started', 'to', 'gain', 'traction', 'like', 'smart', 'prosthetics', 'and', 'implants.', 'These', 'play', 'impeccable', 'role', 'patient', 'management', 'post-surgery', 'or', 'rehabilitation.', 'Demographics,', 'laboratory', 'results', 'and', 'vital', 'signs', 'can', 'also', 'used', 'to', 'predict', 'cardiac', 'arrest,', 'transfer', 'into', 'the', 'intensive', 'care', 'unit,', 'or', 'even', 'death.', 'addition,', 'interpretable', 'machine-learning', 'model', 'can', 'assist', 'anesthesiologists', 'predicting', 'hypoxaemia', 'events', 'during', 'surgery.', 'This', 'suggests', 'that', 'with', 'deep-learning', 'algorithms,', 'raw', 'patient-monitoring', 'data', 'could', 'better', 'used', 'to', 'avoid', 'information', 'overload', 'and', 'alert', 'overload', 'while', 'enabling', 'more', 'accurate', 'clinical', 'prediction', 'and', 'timely', 'decision-making.']\n",
      "['Conclusion']\n",
      "['Considering', 'the', 'vast', 'range', 'of', 'tasks', 'that', 'AI', 'can', 'do,', 'it', 'is', 'evident', 'that', 'it', 'holds', 'deep', 'potential', 'improving', 'patient', 'outcomes', 'to', 'skyrocketing', 'levels.', 'Using', 'sophisticated', 'algorithms', 'AI', 'can', 'bring', 'revolution', 'the', 'healthcare', 'sector.', 'Even', 'after', 'facing', 'challenges', 'like', 'whether', 'the', 'technology', 'will', 'able', 'to', 'deliver', 'the', 'promises,', 'ethical', 'measures,', 'training', 'physicians', 'to', 'use', 'it,', 'standard', 'regulations', 'etc,', 'the', 'role', 'of', 'AI', 'transforming', 'the', 'clinical', 'practices', 'cannot', 'ignored.', 'The', 'biggest', 'challenge', 'is', 'the', 'integration', 'of', 'AI', 'daily', 'practice.', 'All', 'of', 'these', 'can', 'overcome', 'and', 'within', 'that', 'period', 'the', 'technologies', 'will', 'mature', 'making', 'the', 'system', 'far', 'more', 'enhanced', 'and', 'effective.']\n",
      "['We', 'provide', 'intelligence,', 'accelerate', 'innovation', 'and', 'implement', 'technology', 'with', 'extraordinary', 'breadth', 'and', 'depth', 'global', 'insights', 'into', 'the', 'big', 'data,data-driven', 'dashboards,', 'applications', 'development,', 'and', 'information', 'management', 'for', 'organizations', 'through', 'combining', 'unique,', 'specialist', 'services', 'and', 'high-lvel', 'human', 'expertise.']\n",
      "['Contact', 'us:', 'hello@blackcoffer.com']\n",
      "['©', 'All', 'Right', 'Reserved,', 'Blackcoffer(OPC)', 'Pvt.', 'Ltd']\n",
      "['Introduction']\n",
      "['“If', 'kills', '10', 'million', 'people', 'decades,', 'highly', 'infectious', 'virus', 'war.', 'missiles', 'microbes.”', 'Bill', 'Gates’s', 'remarks', 'conference', '2014,', 'world', 'avoided', 'Ebola', 'outbreak.', 'new,', 'unprecedented,', 'invisible', 'virus', 'us,', 'overwhelmed', 'unprepared', 'healthcare', 'system', 'oblivious', 'population.', 'public', 'health', 'emergency', 'demonstrated', 'lack', 'scientific', 'consideration', 'underlined', 'alarming', 'robust', 'innovations', 'health', 'medical', 'facilities.', 'past', 'years,', 'artificial', 'intelligence', 'proven', 'tangible', 'potential', 'healthcare', 'sectors,', 'clinical', 'practices,', 'translational', 'medical', 'biomedical', 'research.']\n",
      "['case', 'detected', 'China', 'December', '31st', '2019,', 'program', 'developed', 'BlueDot', 'alerted', 'world', 'pandemic.', 'quick', 'realise', 'AI’s', 'ability', 'analyse', 'large', 'chunks', 'data', 'detecting', 'patterns', 'identifying', 'tracking', 'carriers', 'virus.']\n",
      "['tracing', 'apps', 'tabs', 'people', 'infected', 'prevent', 'risk', 'cross-infection', 'algorithms', 'track', 'patterns', 'extract', 'features', 'classify', 'categorise', 'them.']\n",
      "['that?']\n",
      "['IBM', 'Watson,', 'sophisticated', 'works', 'cloud', 'computing', 'natural', 'language', 'processing,', 'prominently', 'contributed', 'healthcare', 'sector', 'global', 'level.', 'conversational', 'AI,', '2013,', 'Watson', 'helped', 'recommending', 'treatments', 'patients', 'suffering', 'cancer', 'ensure', 'treatment', 'optimum', 'costs.']\n",
      "['Researchers', 'Google', 'Inc.', 'showed', 'system', 'trained', 'thousands', 'images', 'achieve', 'physician-level', 'sensitivity.']\n",
      "['identifying', 'molecular', 'patterns', 'disease', 'status', 'subtypes,', 'gene', 'expression,', 'protein', 'abundance', 'levels,', 'machine', 'learning', 'methods', 'detect', 'fatal', 'diseases', 'cancer', 'stage.', 'Machine', 'Learning', '(ML)', 'techniques', 'focus', 'analyzing', 'structured', 'data,', 'clustering', 'patients’', 'traits', 'infer', 'probability', 'disease', 'outcomes.', 'patient', 'traits', 'include', 'masses', 'data', 'relating', 'age,', 'gender,', 'disease', 'history,', 'disease-specific', 'data', 'diagnostic', 'imaging', 'gene', 'expressions,', 'etc,', 'ML', 'extract', 'features', 'data', 'inputs', 'constructing', 'data', 'analytical', 'algorithms.']\n",
      "['ML', 'algorithms', 'supervised', 'unsupervised.', 'Unsupervised', 'learning', 'helps', 'extracting', 'features', 'clustering', 'similar', 'features', 'leads', 'detection', 'diseases.', 'Clustering', 'principal', 'component', 'analysis', 'enable', 'grouping', 'clustering', 'similar', 'traits', 'maximize', 'minimize', 'similarity', 'patients', 'clusters.', 'patient', 'traits', 'recorded', 'multiple', 'dimensions,', 'genes,', 'principal', 'component', 'analysis(PCA)', 'creates', 'apparatus', 'reduce', 'dimensions', 'humans', 'alone.']\n",
      "['Supervised', 'learning', 'considers', 'outcomes', 'subjects', 'traits,', 'correlates', 'inputs', 'outputs', 'predict', 'probability', 'clinical', 'event,', 'expected', 'disease', 'level', 'expected', 'survival', 'time,', 'risk', 'Down’s', 'syndrome.']\n",
      "['Biomarker', 'panels', 'detect', 'ovarian', 'cancer,', 'outperformed', 'conventional', 'statistical', 'methods', 'due', 'machine', 'learning.', 'addition', 'this,', 'EHRs', 'Bayesian', 'networks,', 'supervised', 'machine', 'learning', 'algorithms,', 'predict', 'clinical', 'outcomes', 'mortality', 'respectively.']\n",
      "['Unstructured', 'data', 'clinical', 'notes', 'texts', 'converted', 'machine-readable', 'structured', 'data', 'natural', 'language', 'processing(NLP).', 'NLP', 'works', 'components:', 'text', 'processing', 'classification.', 'Text', 'processing', 'helps', 'identifying', 'series', 'disease-relevant', 'keywords', 'clinical', 'notes', 'classification', 'categorized', 'abnormal', 'cases.', 'Chest', 'screening', 'ML', 'NLP', 'helped', 'find', 'abnormalities', 'lungs', 'treatment', 'covid', 'patients.', 'Healthcare', 'organizations', 'NLP-based', 'chatbots', 'increase', 'interactions', 'patients,', 'keeping', 'mental', 'health', 'wellness', 'check.']\n",
      "['Deep', 'learning', 'modern', 'extension', 'classical', 'neural', 'network', 'techniques', 'helps', 'explore', 'complex', 'non-linear', 'patterns', 'data,', 'algorithms', 'convolution', 'neural', 'network,', 'recurrent', 'neural', 'network,', 'deep', 'belief', 'network,', 'deep', 'neural', 'network', 'enables', 'accurate', 'clinical', 'prediction.', 'genome', 'interpretation,', 'deep', 'neural', 'networks', 'surpass', 'conventional', 'methods', 'logistics', 'regression', 'support', 'vector', 'machines.']\n",
      "['Sepsis', 'Watch', 'system', 'trained', 'deep', 'learning', 'algorithms', 'holds', 'capability', 'analyze', '32', 'million', 'data', 'points', 'create', 'patient’s', 'risk', 'score', 'identify', 'stages', 'sepsis.']\n",
      "['method', 'Learning-based', 'Optimization', 'Sampling', 'Pattern(', 'LOUPE)', 'based', 'integrating', 'resolution', 'MRI', 'scans', 'convolutional', 'neural', 'network', 'algorithm,', 'helps', 'creating', 'accurate', 'reconstructions.']\n",
      "['Robotic', 'surgery', 'widely', 'considered', 'delicate', 'surgeries', 'gynaecology', 'prostate', 'surgery.', 'striking', 'balance', 'human', 'decisions', 'precision,', 'robotic', 'surgery', 'reduces', 'surgeon', 'efficiency', 'manually', 'operated', 'console.', 'Thus,', 'autonomous', 'robotic', 'surgery', 'rise', 'inventions', 'robotic', 'silicon', 'fingers', 'mimic', 'sense', 'touch', 'surgeons', 'identify', 'organs,', 'cut', 'tissues,', 'etc.,', 'robotic', 'catheters', 'navigate', 'touching', 'blood,', 'tissue,', 'valve.']\n",
      "['Researchers', 'Children’s', 'National', 'Hospital,', 'Washington', 'developed', 'called', 'Smart', 'Tissue', 'Autonomous', 'Robot', '(STAR),', 'performs', 'colon', 'anastomosis', 'ML-powered', 'suturing', 'tool,', 'automatically', 'detects', 'patient’s', 'breathing', 'pattern', 'apply', 'suture', 'correct', 'point.']\n",
      "['Cloud', 'computing', 'healthcare', 'helped', 'retrieving', 'sharing', 'medical', 'records', 'safely', 'reduction', 'maintenance', 'costs.', 'technology', 'doctors', 'healthcare', 'workers', 'access', 'detailed', 'patient', 'data', 'helps', 'speeding', 'analysis', 'ultimately', 'leading', 'care', 'accurate', 'information,', 'medications,', 'therapies.']\n",
      "['Biomedical', 'research?']\n",
      "['analyze', 'literature', 'readability,', 'concise', 'biomedical', 'research.', 'ML', 'algorithms', 'NLP,', 'accelerate', 'screening', 'indexing', 'biomedical', 'research,', 'ranking', 'literature', 'interest', 'researchers', 'formulate', 'test', 'scientific', 'hypotheses', 'precisely', 'quickly.', 'Taking', 'level,', 'systems', 'computational', 'modelling', 'assistant', '(CMA)', 'helps', 'researchers', 'construct', 'simulation', 'models', 'concepts', 'mind.', 'innovations', 'majorly', 'contributed', 'topics', 'tumour', 'suppressor', 'mechanisms', 'protein-protein', 'interaction', 'information', 'extraction.']\n",
      "['precision', 'medicine']\n",
      "['precision', 'medicine', 'focuses', 'healthcare', 'interventions', 'individuals', 'groups', 'patients', 'based', 'profile,', 'devices', 'pave', 'practice', 'efficiently.', 'ML,', 'complex', 'algorithms', 'large', 'datasets', 'predict', 'create', 'optimal', 'treatment', 'strategy.']\n",
      "['Deep', 'learning', 'neural', 'networks', 'process', 'data', 'healthcare', 'apps', 'close', 'watch', 'patient’s', 'emotional', 'state,', 'food', 'intake,', 'health', 'monitoring.']\n",
      "['“Omics”', 'refers', 'collective', 'technologies', 'exploring', 'roles,', 'relationships', 'branches', 'ending', 'suffix', '“omics”', 'genomics,', 'proteomics,', 'etc.', 'Omics-based', 'tests', 'based', 'machine', 'learning', 'algorithms', 'find', 'correlations', 'predict', 'treatment', 'responses,', 'ultimately', 'creating', 'personalized', 'treatments', 'individual', 'patients.']\n",
      "['helps', 'psychology', 'neuro', 'patients']\n",
      "['psychologists', 'studying', 'creativity,', 'promising', 'classes', 'experiments', 'developing', 'data', 'structures', 'programs', 'exploring', 'theories', 'horizon.', 'Studies', 'show', 'conduct', 'therapy', 'sessions,', 'e-therapy', 'sessions,', 'assessments', 'autonomously,', 'assisting', 'human', 'practitioners', 'before,', 'during,', 'sessions.', 'Detection', 'Computational', 'Analysis', 'Psychological', 'Signal', 'project', 'ML,', 'computer', 'vision,', 'NLP', 'analyze', 'language,', 'physical', 'gestures,', 'social', 'signals', 'identify', 'cues', 'human', 'distress.', 'ground-breaking', 'technology', 'assesses', 'soldiers', 'returning', 'combat', 'recognizes', 'require', 'mental', 'health', 'support.', 'future,', 'combine', 'data', 'captured', 'face-to-face', 'interviews', 'information', 'sleeping,', 'eating,', 'online', 'behaviours', 'complete', 'patient', 'view.']\n",
      "['Stroke', 'identification']\n",
      "['Stroke', 'frequently', 'occurring', 'disease', 'affects', '500', 'million', 'people', 'worldwide.', 'Thrombus,', 'vessel', 'cerebral', 'infarction', 'major', '(about', '85%)', 'stroke', 'occurrence.', 'recent', 'years,', 'techniques', 'numerous', 'stroke-related', 'studies', 'detection', 'timely', 'treatment', 'efficient', 'outcome', 'prediction', 'solve', 'problem.', 'disposal,', 'large', 'amounts', 'data', 'rich', 'information,', 'complications', 'real-life', 'clinical', 'questions', 'addressed', 'arena.', 'Currently,', 'ML', 'algorithms-', 'genetic', 'fuzzy', 'state', 'machine', 'PCA', 'implemented', 'build', 'model', 'building', 'solution.', 'include', 'human', 'activity', 'recognition', 'stage', 'stroke', 'onset', 'detection', 'stage.', 'alert', 'stroke', 'message', 'activated', 'movement', 'significantly', 'pattern', 'recorded.', 'ML', 'methods', 'applied', 'neuroimaging', 'data', 'assist', 'disease', 'evaluation', 'predicting', 'stroke', 'treatment', 'diagnosis.']\n",
      "['Patient', 'Monitoring']\n",
      "['Today,', 'market', 'AI-based', 'patient', 'monitoring', 'impressive', 'monetarily', 'enticing.', 'evolving', 'artificial', 'sensors,', 'smart', 'technologies', 'explores', 'brain-computer', 'interfaces', 'nanorobotics.', 'Companies', 'smart-watches', 'engaged', 'people', 'perform', 'remote', 'monitoring', '“patients”.', 'start', 'wearable', 'embedded', 'sensors,', 'glucose', 'monitors,', 'pulse', 'monitors,', 'oximeters,', 'ECG', 'monitors.', 'patient', 'monitoring', 'crucial,', 'finds', 'numerous', 'applications', 'chronic', 'conditions,', 'intensive', 'care', 'units,', 'operating', 'rooms,', 'emergency', 'rooms,', 'cardiac', 'timeless', 'clinical', 'decision-making', 'measured', 'seconds.', 'advances', 'started', 'traction', 'smart', 'prosthetics', 'implants.', 'play', 'impeccable', 'role', 'patient', 'management', 'post-surgery', 'rehabilitation.', 'Demographics,', 'laboratory', 'results', 'vital', 'signs', 'predict', 'cardiac', 'arrest,', 'transfer', 'intensive', 'care', 'unit,', 'death.', 'addition,', 'interpretable', 'machine-learning', 'model', 'assist', 'anesthesiologists', 'predicting', 'hypoxaemia', 'events', 'surgery.', 'suggests', 'deep-learning', 'algorithms,', 'raw', 'patient-monitoring', 'data', 'avoid', 'information', 'overload', 'alert', 'overload', 'enabling', 'accurate', 'clinical', 'prediction', 'timely', 'decision-making.']\n",
      "['Conclusion']\n",
      "['vast', 'range', 'tasks', 'do,', 'evident', 'holds', 'deep', 'potential', 'improving', 'patient', 'outcomes', 'skyrocketing', 'levels.', 'sophisticated', 'algorithms', 'bring', 'revolution', 'healthcare', 'sector.', 'facing', 'challenges', 'technology', 'deliver', 'promises,', 'ethical', 'measures,', 'training', 'physicians', 'it,', 'standard', 'regulations', 'etc,', 'role', 'transforming', 'clinical', 'practices', 'ignored.', 'biggest', 'challenge', 'integration', 'daily', 'practice.', 'overcome', 'period', 'technologies', 'mature', 'making', 'system', 'enhanced', 'effective.']\n",
      "['intelligence,', 'accelerate', 'innovation', 'implement', 'technology', 'extraordinary', 'breadth', 'depth', 'global', 'insights', 'big', 'data,data-driven', 'dashboards,', 'applications', 'development,', 'information', 'management', 'organizations', 'combining', 'unique,', 'specialist', 'services', 'high-lvel', 'human', 'expertise.']\n",
      "['Contact', 'us:', 'hello@blackcoffer.com']\n",
      "['©', 'Reserved,', 'Blackcoffer(OPC)', 'Pvt.']\n"
     ]
    }
   ],
   "source": [
    "for p in filter_paragraph:\n",
    "        print(p.split())\n",
    "        # print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Positive score</th>\n",
       "      <th>Negative score</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex Words</th>\n",
       "      <th>FOG Index</th>\n",
       "      <th>Average Number of Words per Sentence</th>\n",
       "      <th>Complex Word Count</th>\n",
       "      <th>Syllables per Word</th>\n",
       "      <th>Personal Pronouns Count</th>\n",
       "      <th>Average Word Length</th>\n",
       "      <th>clened words</th>\n",
       "      <th>No. of Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>476</td>\n",
       "      <td>182</td>\n",
       "      <td>18.860269</td>\n",
       "      <td>30.768544</td>\n",
       "      <td>19.851525</td>\n",
       "      <td>45.726531</td>\n",
       "      <td>3447</td>\n",
       "      <td>2.061412</td>\n",
       "      <td>19</td>\n",
       "      <td>2.061412</td>\n",
       "      <td>9549</td>\n",
       "      <td>11203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>383</td>\n",
       "      <td>194</td>\n",
       "      <td>15.027682</td>\n",
       "      <td>19.997697</td>\n",
       "      <td>14.010152</td>\n",
       "      <td>65.308271</td>\n",
       "      <td>1737</td>\n",
       "      <td>1.775616</td>\n",
       "      <td>46</td>\n",
       "      <td>1.775616</td>\n",
       "      <td>6845</td>\n",
       "      <td>8686.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>455</td>\n",
       "      <td>203</td>\n",
       "      <td>15.923896</td>\n",
       "      <td>29.917798</td>\n",
       "      <td>18.336678</td>\n",
       "      <td>51.536946</td>\n",
       "      <td>3130</td>\n",
       "      <td>2.029344</td>\n",
       "      <td>27</td>\n",
       "      <td>2.029344</td>\n",
       "      <td>8503</td>\n",
       "      <td>10462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>371</td>\n",
       "      <td>97</td>\n",
       "      <td>13.199730</td>\n",
       "      <td>20.611389</td>\n",
       "      <td>13.524448</td>\n",
       "      <td>43.665179</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.818321</td>\n",
       "      <td>99</td>\n",
       "      <td>1.818321</td>\n",
       "      <td>7389</td>\n",
       "      <td>9781.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>350</td>\n",
       "      <td>131</td>\n",
       "      <td>17.447587</td>\n",
       "      <td>23.221438</td>\n",
       "      <td>16.267610</td>\n",
       "      <td>46.812500</td>\n",
       "      <td>2435</td>\n",
       "      <td>1.863437</td>\n",
       "      <td>92</td>\n",
       "      <td>1.863437</td>\n",
       "      <td>8298</td>\n",
       "      <td>10486.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>183</td>\n",
       "      <td>161</td>\n",
       "      <td>14.718750</td>\n",
       "      <td>27.087757</td>\n",
       "      <td>16.722603</td>\n",
       "      <td>44.857143</td>\n",
       "      <td>1531</td>\n",
       "      <td>1.951522</td>\n",
       "      <td>40</td>\n",
       "      <td>1.951522</td>\n",
       "      <td>4553</td>\n",
       "      <td>5652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>241</td>\n",
       "      <td>56</td>\n",
       "      <td>18.706522</td>\n",
       "      <td>25.697269</td>\n",
       "      <td>17.761516</td>\n",
       "      <td>65.561905</td>\n",
       "      <td>1769</td>\n",
       "      <td>1.920250</td>\n",
       "      <td>22</td>\n",
       "      <td>1.920250</td>\n",
       "      <td>5605</td>\n",
       "      <td>6884.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>213</td>\n",
       "      <td>245</td>\n",
       "      <td>14.480573</td>\n",
       "      <td>28.103375</td>\n",
       "      <td>17.033579</td>\n",
       "      <td>53.240602</td>\n",
       "      <td>1990</td>\n",
       "      <td>1.950290</td>\n",
       "      <td>23</td>\n",
       "      <td>1.950290</td>\n",
       "      <td>5753</td>\n",
       "      <td>7081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>193</td>\n",
       "      <td>14</td>\n",
       "      <td>17.755981</td>\n",
       "      <td>31.985988</td>\n",
       "      <td>19.896787</td>\n",
       "      <td>37.867347</td>\n",
       "      <td>1187</td>\n",
       "      <td>2.082996</td>\n",
       "      <td>12</td>\n",
       "      <td>2.082996</td>\n",
       "      <td>2937</td>\n",
       "      <td>3711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>283</td>\n",
       "      <td>243</td>\n",
       "      <td>13.448133</td>\n",
       "      <td>26.164764</td>\n",
       "      <td>15.845159</td>\n",
       "      <td>84.181818</td>\n",
       "      <td>1696</td>\n",
       "      <td>1.948164</td>\n",
       "      <td>50</td>\n",
       "      <td>1.948164</td>\n",
       "      <td>5034</td>\n",
       "      <td>6482.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL Positive score  \\\n",
       "0    https://insights.blackcoffer.com/ai-in-healthc...            476   \n",
       "1    https://insights.blackcoffer.com/what-if-the-c...            383   \n",
       "2    https://insights.blackcoffer.com/what-jobs-wil...            455   \n",
       "3    https://insights.blackcoffer.com/will-machine-...            371   \n",
       "4    https://insights.blackcoffer.com/will-ai-repla...            350   \n",
       "..                                                 ...            ...   \n",
       "109  https://insights.blackcoffer.com/blockchain-fo...            183   \n",
       "110  https://insights.blackcoffer.com/the-future-of...            241   \n",
       "111  https://insights.blackcoffer.com/big-data-anal...            213   \n",
       "112  https://insights.blackcoffer.com/business-anal...            193   \n",
       "113  https://insights.blackcoffer.com/challenges-an...            283   \n",
       "\n",
       "    Negative score  Average Sentence Length  Percentage of Complex Words  \\\n",
       "0              182                18.860269                    30.768544   \n",
       "1              194                15.027682                    19.997697   \n",
       "2              203                15.923896                    29.917798   \n",
       "3               97                13.199730                    20.611389   \n",
       "4              131                17.447587                    23.221438   \n",
       "..             ...                      ...                          ...   \n",
       "109            161                14.718750                    27.087757   \n",
       "110             56                18.706522                    25.697269   \n",
       "111            245                14.480573                    28.103375   \n",
       "112             14                17.755981                    31.985988   \n",
       "113            243                13.448133                    26.164764   \n",
       "\n",
       "     FOG Index  Average Number of Words per Sentence Complex Word Count  \\\n",
       "0    19.851525                             45.726531               3447   \n",
       "1    14.010152                             65.308271               1737   \n",
       "2    18.336678                             51.536946               3130   \n",
       "3    13.524448                             43.665179               2016   \n",
       "4    16.267610                             46.812500               2435   \n",
       "..         ...                                   ...                ...   \n",
       "109  16.722603                             44.857143               1531   \n",
       "110  17.761516                             65.561905               1769   \n",
       "111  17.033579                             53.240602               1990   \n",
       "112  19.896787                             37.867347               1187   \n",
       "113  15.845159                             84.181818               1696   \n",
       "\n",
       "     Syllables per Word Personal Pronouns Count  Average Word Length  \\\n",
       "0              2.061412                      19             2.061412   \n",
       "1              1.775616                      46             1.775616   \n",
       "2              2.029344                      27             2.029344   \n",
       "3              1.818321                      99             1.818321   \n",
       "4              1.863437                      92             1.863437   \n",
       "..                  ...                     ...                  ...   \n",
       "109            1.951522                      40             1.951522   \n",
       "110            1.920250                      22             1.920250   \n",
       "111            1.950290                      23             1.950290   \n",
       "112            2.082996                      12             2.082996   \n",
       "113            1.948164                      50             1.948164   \n",
       "\n",
       "    clened words  No. of Words  \n",
       "0           9549       11203.0  \n",
       "1           6845        8686.0  \n",
       "2           8503       10462.0  \n",
       "3           7389        9781.0  \n",
       "4           8298       10486.0  \n",
       "..           ...           ...  \n",
       "109         4553        5652.0  \n",
       "110         5605        6884.0  \n",
       "111         5753        7081.0  \n",
       "112         2937        3711.0  \n",
       "113         5034        6482.0  \n",
       "\n",
       "[114 rows x 13 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
